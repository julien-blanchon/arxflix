{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import Voice, VoiceSettings, play, stream, save\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from IPython.display import Audio as IPAudio\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import numpy as np\n",
    "from typing import Literal, Iterator\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torch\n",
    "import torchaudio\n",
    "import srt\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEVENLABS_API_KEY = \"c973ad05989d6e4ce28a9acf65238957\"\n",
    "ELEVENLABS_VOICE = Voice(\n",
    "    voice_id='lxYfHSkYm1EzQzGhdbfc',\n",
    "    settings=VoiceSettings(stability=0.35, similarity_boost=0.8, style=0.0, use_speaker_boost=True)\n",
    ")\n",
    "ELEVENLABS_MODEL = \"eleven_turbo_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Caption:\n",
    "    word: str\n",
    "    start: float\n",
    "    end: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RichContent:\n",
    "    content: str\n",
    "    start: float | None = None\n",
    "    end: float | None = None\n",
    "\n",
    "@dataclass\n",
    "class Figure(RichContent):\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    content: str\n",
    "    audio: bytes | Iterator[bytes] | None = None\n",
    "    audio_path: str | None = None\n",
    "    captions: list[Caption] | None = None\n",
    "    start: float | None = None\n",
    "    end: float | None = None\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Equation(RichContent):\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Headline(RichContent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = open('./paper/paper.md', 'r').read()\n",
    "script = open('./script/script.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Headline: Michelangelo: Conditional 3D Shape Generation\\n\\\\Text: Welcome back to Arxflix! Today, we’re diving into an intriguing new paper titled \"Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation\". This research looks at a novel approach for generating 3D shapes conditioned on 2D images or texts, which is a significant breakthrough in the field of deep learning and 3D modeling.\\n\\\\Figure: https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x1.png\\n\\\\Text: Here’s an overview of the Michelangelo pipeline showing the two key models: the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) and the Aligned Shape Latent Diffusion Model (ASLDM). These models work together to bridge the distribution gap between 3D shapes and 2D images or texts.\\n\\\\Headline: The Challenge\\n\\\\Text: The main challenge addressed in this paper is the significant distribution gap between 3D shapes and their 2D or textual descriptions. Directly mapping these modalities can lead to inconsistent and poor-quality 3D shapes. Michelangelo tackles this through the concept of alignment before generation.\\n\\\\Figure: https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x2.png\\n\\\\Text: Here’s a comparison showcasing how Michelangelo maintains detailed 3D shape generation compared to other methods. Notice the smooth surfaces and detailed structure, which contrast with the oversimplified and noisy results from other models.\\n\\\\Headline: Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)\\n\\\\Text: SITA-VAE creates a shared latent space for 3D shapes, images, and texts. It uses a CLIP-based encoder to align images and texts and a trainable 3D shape encoder to map 3D shapes into this shared space. The transformer-based neural field decoder then ensures high-quality 3D reconstruction.\\n\\\\Equation: e_s = \\\\mathcal{F}_s(\\\\mathbf{E}_s), e_i = \\\\mathcal{F}_i(\\\\mathbf{E}_i), e_t = \\\\mathcal{F}_t(\\\\mathbf{E}_t)\\n\\\\Text: This equation shows how the embeddings for shapes, images, and texts are projected into the aligned space. A contrastive loss is applied to ensure these embeddings are well-aligned.\\n\\\\Headline: Aligned Shape Latent Diffusion Model (ASLDM)\\n\\\\Text: ASLDM builds on the success of Latent Diffusion Models. It maps 2D images and texts to the shape latent embeddings within the aligned space. This model leverages a UNet-like architecture to generate consistent and high-quality 3D shapes.\\n\\\\Equation: \\\\mathcal{L} = \\\\| \\\\mathbf{E}_s^{(0)} - \\\\mathbf{E}_s^{(t)} \\\\|\\n\\\\Text: This objective function ensures the generative process maintains high fidelity throughout the diffusion steps.\\n\\n\\\\Figure: https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x3.png\\n\\\\Text: This figure illustrates the text-conditioned 3D shape generation capability of Michelangelo. Even abstract or detailed textual descriptions result in smooth, high-fidelity 3D shapes, proving the model’s efficiency in capturing and translating semantic details.\\n\\\\Headline: Experimental Validation\\n\\\\Text: Extensive experiments were conducted using the ShapeNet dataset and a customized 3D Cartoon Monster dataset. Metrics like Intersection Over Union (IoU) and newly proposed Shape-Image Score (SI-S) and Shape-Text Score (ST-S) were used to validate the model’s performance.\\n\\\\Figure: https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x4.png\\n\\\\Text: Here’s an ablation study showcasing the importance of training the model in the aligned space. Training in this manner leads to samples that are significantly closer to the given text and image conditions.\\n\\\\Headline: Conclusion and Future Work\\n\\\\Text: Michelangelo has proven effective in aligning and generating 3D shapes from 2D images and texts. The results show higher quality and diversity of 3D shape generation compared to other methods. Future work could address the need for ground truth 3D shapes for training, possibly leveraging differentiable rendering from multi-view images.\\n\\\\Text: Thanks for watching Arxflix! Don\\'t forget to like and subscribe for more deep dives into the latest research papers. Leave a comment below on what paper you would like us to cover next!'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_script(script: str) -> list[RichContent | Text]:\n",
    "    lines = script.split('\\n')\n",
    "    content = []\n",
    "    for line in lines:\n",
    "        if line.startswith(r'\\Figure: '):\n",
    "            figure_content = line.replace(r'\\Figure: ', '')\n",
    "            figure = Figure(content=figure_content)\n",
    "            content.append(figure)\n",
    "        elif line.startswith(r'\\Text: '):\n",
    "            text_content = line.replace(r'\\Text: ', '')\n",
    "            text = Text(content=text_content)\n",
    "            content.append(text)\n",
    "        elif line.startswith(r'\\Equation: '):\n",
    "            equation_content = line.replace(r'\\Equation: ', '')\n",
    "            equation = Equation(content=equation_content)\n",
    "            content.append(equation)\n",
    "        elif line.startswith(r'\\Headline: '):\n",
    "            headline_content = line.replace(r'\\Headline: ', '')\n",
    "            headline = Headline(content=headline_content)\n",
    "            content.append(headline)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Headline(content='Michelangelo: Conditional 3D Shape Generation', start=0.0, end=24.372244897959185),\n",
       " Text(content='Welcome back to Arxflix! Today, we’re diving into an intriguing new paper titled \"Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation\". This research looks at a novel approach for generating 3D shapes conditioned on 2D images or texts, which is a significant breakthrough in the field of deep learning and 3D modeling.', audio=<generator object TextToSpeechClient.convert at 0x152503dc0>, audio_path='./audio/text_1.wav', captions=[Caption(word='Welcome', start=0.0, end=0.2), Caption(word='back', start=0.2, end=0.56), Caption(word='to', start=0.56, end=0.76), Caption(word='ARXFlicks.', start=0.76, end=1.3), Caption(word='Today', start=2.0, end=2.18), Caption(word=\"we're\", start=2.18, end=2.44), Caption(word='diving', start=2.44, end=2.6), Caption(word='into', start=2.6, end=3.04), Caption(word='an', start=3.04, end=3.14), Caption(word='intriguing', start=3.14, end=3.46), Caption(word='new', start=3.46, end=3.82), Caption(word='paper', start=3.82, end=4.16), Caption(word='titled', start=4.16, end=4.6), Caption(word='Michelangelo,', start=4.6, end=5.58), Caption(word='Conditional', start=6.18, end=6.64), Caption(word='3D', start=6.64, end=7.26), Caption(word='Shape', start=7.26, end=7.56), Caption(word='Generation', start=7.56, end=8.0), Caption(word='based', start=8.0, end=8.6), Caption(word='on', start=8.6, end=8.92), Caption(word='shape', start=8.92, end=9.32), Caption(word='image', start=9.32, end=9.74), Caption(word='text', start=9.74, end=10.2), Caption(word='aligned', start=10.2, end=10.64), Caption(word='latent', start=10.64, end=11.18), Caption(word='representation.', start=11.18, end=11.88), Caption(word='This', start=12.88, end=13.02), Caption(word='research', start=13.02, end=13.44), Caption(word='looks', start=13.44, end=13.7), Caption(word='at', start=13.7, end=13.88), Caption(word='a', start=13.88, end=13.94), Caption(word='novel', start=13.94, end=14.14), Caption(word='approach', start=14.14, end=14.72), Caption(word='for', start=14.72, end=14.94), Caption(word='generating', start=14.94, end=15.24), Caption(word='3D', start=15.24, end=16.18), Caption(word='shapes', start=16.18, end=16.56), Caption(word='conditioned', start=16.56, end=16.92), Caption(word='on', start=16.92, end=17.36), Caption(word='2D', start=17.36, end=17.9), Caption(word='images', start=17.9, end=18.28), Caption(word='or', start=18.28, end=18.6), Caption(word='texts,', start=18.6, end=18.86), Caption(word='which', start=19.52, end=19.74), Caption(word='is', start=19.74, end=19.86), Caption(word='a', start=19.86, end=19.98), Caption(word='significant', start=19.98, end=20.3), Caption(word='breakthrough', start=20.3, end=20.74), Caption(word='in', start=20.74, end=21.16), Caption(word='the', start=21.16, end=21.24), Caption(word='field', start=21.24, end=21.46), Caption(word='of', start=21.46, end=21.62), Caption(word='deep', start=21.62, end=21.78), Caption(word='learning', start=21.78, end=22.12), Caption(word='and', start=22.12, end=22.74), Caption(word='3D', start=22.74, end=23.28), Caption(word='modeling.', start=23.28, end=23.66)], start=0, end=24.372244897959185),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x1.png', start=24.372244897959185, end=47.25551020408163),\n",
       " Text(content='Here’s an overview of the Michelangelo pipeline showing the two key models: the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) and the Aligned Shape Latent Diffusion Model (ASLDM). These models work together to bridge the distribution gap between 3D shapes and 2D images or texts.', audio=<generator object TextToSpeechClient.convert at 0x2bc42cc40>, audio_path='./audio/text_3.wav', captions=[Caption(word=\"Here's\", start=24.372244897959185, end=24.672244897959185), Caption(word='an', start=24.672244897959185, end=24.772244897959183), Caption(word='overview', start=24.772244897959183, end=25.172244897959185), Caption(word='of', start=25.172244897959185, end=25.552244897959184), Caption(word='the', start=25.552244897959184, end=25.672244897959185), Caption(word='Michelangelo', start=25.672244897959185, end=26.412244897959184), Caption(word='pipeline', start=26.412244897959184, end=26.792244897959186), Caption(word='showing', start=26.792244897959186, end=27.892244897959184), Caption(word='the', start=27.892244897959184, end=28.152244897959186), Caption(word='two', start=28.152244897959186, end=28.392244897959184), Caption(word='key', start=28.392244897959184, end=28.752244897959184), Caption(word='models.', start=28.752244897959184, end=29.192244897959185), Caption(word='The', start=30.112244897959187, end=30.132244897959183), Caption(word='shape', start=30.132244897959183, end=30.412244897959184), Caption(word='image', start=30.412244897959184, end=30.832244897959185), Caption(word='text', start=30.832244897959185, end=31.252244897959184), Caption(word='aligned', start=31.252244897959184, end=31.732244897959184), Caption(word='variational', start=31.732244897959184, end=32.81224489795918), Caption(word='auto', start=32.81224489795918, end=33.21224489795918), Caption(word='encoder,', start=33.21224489795918, end=33.81224489795918), Caption(word='CITA', start=34.152244897959186, end=34.512244897959185), Caption(word='-VAT', start=34.512244897959185, end=34.792244897959186), Caption(word='-E,', start=34.792244897959186, end=35.052244897959184), Caption(word='and', start=35.652244897959186, end=35.75224489795919), Caption(word='the', start=35.75224489795919, end=35.872244897959185), Caption(word='aligned', start=35.872244897959185, end=36.292244897959186), Caption(word='shape', start=36.292244897959186, end=36.89224489795919), Caption(word='latent', start=36.89224489795919, end=37.332244897959185), Caption(word='diffusion', start=37.332244897959185, end=37.71224489795918), Caption(word='model,', start=37.71224489795918, end=38.43224489795919), Caption(word='ASLDM.', start=38.832244897959185, end=39.89224489795919), Caption(word='These', start=40.63224489795918, end=40.71224489795918), Caption(word='models', start=40.71224489795918, end=41.07224489795918), Caption(word='work', start=41.07224489795918, end=41.39224489795919), Caption(word='together', start=41.39224489795919, end=41.71224489795918), Caption(word='to', start=41.71224489795918, end=42.052244897959184), Caption(word='bridge', start=42.052244897959184, end=42.232244897959184), Caption(word='the', start=42.232244897959184, end=42.512244897959185), Caption(word='distribution', start=42.512244897959185, end=42.95224489795918), Caption(word='gap', start=42.95224489795918, end=43.372244897959185), Caption(word='between', start=43.372244897959185, end=43.77224489795918), Caption(word='3D', start=43.77224489795918, end=44.39224489795919), Caption(word='shapes', start=44.39224489795919, end=44.732244897959184), Caption(word='and', start=44.732244897959184, end=45.07224489795918), Caption(word='2D', start=45.07224489795918, end=45.59224489795918), Caption(word='images', start=45.59224489795918, end=45.95224489795918), Caption(word='or', start=45.95224489795918, end=46.292244897959186), Caption(word='texts.', start=46.292244897959186, end=46.53224489795919)], start=24.372244897959185, end=47.25551020408163),\n",
       " Headline(content='The Challenge', start=47.25551020408163, end=66.66448979591837),\n",
       " Text(content='The main challenge addressed in this paper is the significant distribution gap between 3D shapes and their 2D or textual descriptions. Directly mapping these modalities can lead to inconsistent and poor-quality 3D shapes. Michelangelo tackles this through the concept of alignment before generation.', audio=<generator object TextToSpeechClient.convert at 0x2bc42cac0>, audio_path='./audio/text_5.wav', captions=[Caption(word='The', start=47.25551020408163, end=47.41551020408163), Caption(word='main', start=47.41551020408163, end=47.57551020408163), Caption(word='challenge', start=47.57551020408163, end=47.97551020408163), Caption(word='addressed', start=47.97551020408163, end=48.275510204081634), Caption(word='in', start=48.275510204081634, end=48.57551020408163), Caption(word='this', start=48.57551020408163, end=48.67551020408163), Caption(word='paper', start=48.67551020408163, end=49.03551020408163), Caption(word='is', start=49.03551020408163, end=49.275510204081634), Caption(word='the', start=49.275510204081634, end=49.41551020408163), Caption(word='significant', start=49.41551020408163, end=49.75551020408163), Caption(word='distribution', start=49.75551020408163, end=50.37551020408163), Caption(word='gap', start=50.37551020408163, end=50.955510204081634), Caption(word='between', start=50.955510204081634, end=51.33551020408163), Caption(word='3D', start=51.33551020408163, end=51.99551020408163), Caption(word='shapes', start=51.99551020408163, end=52.29551020408163), Caption(word='and', start=52.29551020408163, end=52.735510204081635), Caption(word='their', start=52.735510204081635, end=52.87551020408163), Caption(word='2D', start=52.87551020408163, end=53.53551020408163), Caption(word='or', start=53.53551020408163, end=54.05551020408163), Caption(word='textual', start=54.05551020408163, end=54.35551020408163), Caption(word='descriptions.', start=54.35551020408163, end=54.83551020408163), Caption(word='Directly', start=55.93551020408163, end=56.33551020408163), Caption(word='mapping', start=56.33551020408163, end=56.49551020408163), Caption(word='these', start=56.49551020408163, end=56.87551020408163), Caption(word='modalities', start=56.87551020408163, end=57.27551020408163), Caption(word='can', start=57.27551020408163, end=57.79551020408163), Caption(word='lead', start=57.79551020408163, end=58.01551020408163), Caption(word='to', start=58.01551020408163, end=58.17551020408163), Caption(word='inconsistent', start=58.17551020408163, end=58.71551020408163), Caption(word='and', start=58.71551020408163, end=59.15551020408163), Caption(word='poor', start=59.15551020408163, end=59.39551020408163), Caption(word='quality', start=59.39551020408163, end=59.735510204081635), Caption(word='3D', start=59.735510204081635, end=60.53551020408163), Caption(word='shapes.', start=60.53551020408163, end=61.03551020408163), Caption(word='Michelangelo', start=62.01551020408163, end=62.455510204081634), Caption(word='tackles', start=62.455510204081634, end=62.89551020408163), Caption(word='this', start=62.89551020408163, end=63.375510204081635), Caption(word='through', start=63.375510204081635, end=63.57551020408163), Caption(word='the', start=63.57551020408163, end=63.75551020408163), Caption(word='concept', start=63.75551020408163, end=64.15551020408162), Caption(word='of', start=64.15551020408162, end=64.45551020408163), Caption(word='alignment', start=64.45551020408163, end=64.73551020408163), Caption(word='before', start=64.73551020408163, end=65.35551020408164), Caption(word='generation.', start=65.35551020408164, end=65.85551020408164)], start=47.25551020408163, end=66.66448979591837),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x2.png', start=66.66448979591837, end=82.49469387755103),\n",
       " Text(content='Here’s a comparison showcasing how Michelangelo maintains detailed 3D shape generation compared to other methods. Notice the smooth surfaces and detailed structure, which contrast with the oversimplified and noisy results from other models.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d840>, audio_path='./audio/text_7.wav', captions=[Caption(word=\"Here's\", start=66.66448979591837, end=67.00448979591837), Caption(word='a', start=67.00448979591837, end=67.08448979591837), Caption(word='comparison', start=67.08448979591837, end=67.58448979591837), Caption(word='showcasing', start=67.58448979591837, end=68.06448979591838), Caption(word='how', start=68.06448979591838, end=68.54448979591837), Caption(word='Michelangelo', start=68.54448979591837, end=69.22448979591837), Caption(word='maintains', start=69.22448979591837, end=69.62448979591836), Caption(word='detailed', start=69.62448979591836, end=70.26448979591837), Caption(word='3D', start=70.26448979591837, end=71.20448979591838), Caption(word='-shaped', start=71.20448979591838, end=71.50448979591837), Caption(word='generation', start=71.50448979591837, end=72.10448979591837), Caption(word='compared', start=72.10448979591837, end=72.86448979591837), Caption(word='to', start=72.86448979591837, end=73.16448979591837), Caption(word='other', start=73.16448979591837, end=73.36448979591837), Caption(word='methods.', start=73.36448979591837, end=73.76448979591837), Caption(word='Notice', start=74.36448979591837, end=74.60448979591837), Caption(word='the', start=74.60448979591837, end=74.90448979591837), Caption(word='smooth', start=74.90448979591837, end=75.18448979591837), Caption(word='surfaces', start=75.18448979591837, end=75.86448979591837), Caption(word='and', start=75.86448979591837, end=76.22448979591837), Caption(word='detailed', start=76.22448979591837, end=76.48448979591836), Caption(word='structure,', start=76.48448979591836, end=77.10448979591837), Caption(word='which', start=77.52448979591837, end=77.82448979591837), Caption(word='contrast', start=77.82448979591837, end=78.22448979591837), Caption(word='with', start=78.22448979591837, end=78.74448979591837), Caption(word='the', start=78.74448979591837, end=78.86448979591837), Caption(word='oversimplified', start=78.86448979591837, end=79.84448979591838), Caption(word='and', start=79.84448979591838, end=80.04448979591837), Caption(word='noisy', start=80.04448979591837, end=80.32448979591837), Caption(word='results', start=80.32448979591837, end=80.84448979591838), Caption(word='from', start=80.84448979591838, end=81.20448979591836), Caption(word='other', start=81.20448979591836, end=81.46448979591837), Caption(word='models.', start=81.46448979591837, end=81.84448979591838)], start=66.66448979591837, end=82.49469387755103),\n",
       " Headline(content='Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)', start=82.49469387755103, end=104.17632653061226),\n",
       " Text(content='SITA-VAE creates a shared latent space for 3D shapes, images, and texts. It uses a CLIP-based encoder to align images and texts and a trainable 3D shape encoder to map 3D shapes into this shared space. The transformer-based neural field decoder then ensures high-quality 3D reconstruction.', audio=<generator object TextToSpeechClient.convert at 0x2bc42db40>, audio_path='./audio/text_9.wav', captions=[Caption(word='CEDA', start=82.49469387755103, end=82.85469387755103), Caption(word='-Vay', start=82.85469387755103, end=83.09469387755102), Caption(word='creates', start=83.09469387755102, end=83.47469387755103), Caption(word='a', start=83.47469387755103, end=83.63469387755103), Caption(word='shared', start=83.63469387755103, end=83.95469387755102), Caption(word='latent', start=83.95469387755102, end=84.17469387755104), Caption(word='space', start=84.17469387755104, end=84.57469387755103), Caption(word='for', start=84.57469387755103, end=84.89469387755103), Caption(word='3D', start=84.89469387755103, end=85.53469387755104), Caption(word='shapes,', start=85.53469387755104, end=85.87469387755102), Caption(word='images,', start=86.19469387755103, end=86.31469387755102), Caption(word='and', start=86.31469387755102, end=86.79469387755103), Caption(word='texts.', start=86.79469387755103, end=87.03469387755104), Caption(word='It', start=87.93469387755103, end=87.99469387755103), Caption(word='uses', start=87.99469387755103, end=88.27469387755103), Caption(word='a', start=88.27469387755103, end=88.47469387755103), Caption(word='clip', start=88.47469387755103, end=88.65469387755103), Caption(word='-based', start=88.65469387755103, end=89.03469387755104), Caption(word='encoder', start=89.03469387755104, end=89.47469387755103), Caption(word='to', start=89.47469387755103, end=90.17469387755102), Caption(word='align', start=90.17469387755102, end=90.37469387755102), Caption(word='images', start=90.37469387755102, end=90.87469387755102), Caption(word='and', start=90.87469387755102, end=91.19469387755103), Caption(word='texts', start=91.19469387755103, end=91.49469387755103), Caption(word='and', start=91.49469387755103, end=92.17469387755102), Caption(word='a', start=92.17469387755102, end=92.25469387755103), Caption(word='trainable', start=92.25469387755103, end=92.67469387755102), Caption(word='3D', start=92.67469387755102, end=93.35469387755103), Caption(word='shape', start=93.35469387755103, end=93.65469387755103), Caption(word='encoder', start=93.65469387755103, end=94.21469387755103), Caption(word='to', start=94.21469387755103, end=94.77469387755103), Caption(word='map', start=94.77469387755103, end=94.93469387755103), Caption(word='3D', start=94.93469387755103, end=95.53469387755104), Caption(word='shapes', start=95.53469387755104, end=95.87469387755102), Caption(word='into', start=95.87469387755102, end=96.29469387755103), Caption(word='this', start=96.29469387755103, end=96.55469387755103), Caption(word='shared', start=96.55469387755103, end=97.07469387755103), Caption(word='space.', start=97.07469387755103, end=97.59469387755102), Caption(word='The', start=98.41469387755103, end=98.45469387755102), Caption(word='transformer', start=98.45469387755102, end=98.89469387755102), Caption(word='-based', start=98.89469387755102, end=99.39469387755102), Caption(word='neural', start=99.39469387755102, end=99.71469387755103), Caption(word='field', start=99.71469387755103, end=100.17469387755102), Caption(word='decoder', start=100.17469387755102, end=100.65469387755103), Caption(word='then', start=100.65469387755103, end=101.01469387755103), Caption(word='ensures', start=101.01469387755103, end=101.33469387755103), Caption(word='high', start=101.33469387755103, end=101.73469387755102), Caption(word='-quality', start=101.73469387755102, end=102.11469387755103), Caption(word='3D', start=102.11469387755103, end=102.83469387755103), Caption(word='reconstruction.', start=102.83469387755103, end=103.25469387755103)], start=82.49469387755103, end=104.17632653061226),\n",
       " Equation(content='e_s = \\\\mathcal{F}_s(\\\\mathbf{E}_s), e_i = \\\\mathcal{F}_i(\\\\mathbf{E}_i), e_t = \\\\mathcal{F}_t(\\\\mathbf{E}_t)', start=104.17632653061226, end=115.06938775510206),\n",
       " Text(content='This equation shows how the embeddings for shapes, images, and texts are projected into the aligned space. A contrastive loss is applied to ensure these embeddings are well-aligned.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d6c0>, audio_path='./audio/text_11.wav', captions=[Caption(word='This', start=104.17632653061226, end=104.35632653061226), Caption(word='equation', start=104.35632653061226, end=104.71632653061226), Caption(word='shows', start=104.71632653061226, end=105.13632653061225), Caption(word='how', start=105.13632653061225, end=105.37632653061226), Caption(word='the', start=105.37632653061226, end=105.49632653061225), Caption(word='embeddings', start=105.49632653061225, end=105.99632653061225), Caption(word='for', start=105.99632653061225, end=106.15632653061226), Caption(word='shapes,', start=106.15632653061226, end=106.51632653061226), Caption(word='images,', start=107.01632653061226, end=107.19632653061225), Caption(word='and', start=107.61632653061226, end=107.63632653061225), Caption(word='texts', start=107.63632653061225, end=107.95632653061226), Caption(word='are', start=107.95632653061226, end=108.41632653061225), Caption(word='projected', start=108.41632653061225, end=108.75632653061226), Caption(word='into', start=108.75632653061226, end=109.27632653061225), Caption(word='the', start=109.27632653061225, end=109.41632653061225), Caption(word='aligned', start=109.41632653061225, end=109.63632653061225), Caption(word='space.', start=109.63632653061225, end=110.17632653061226), Caption(word='A', start=111.01632653061226, end=111.03632653061226), Caption(word='contrast', start=111.03632653061226, end=111.29632653061226), Caption(word='of', start=111.29632653061226, end=111.65632653061226), Caption(word='loss', start=111.65632653061226, end=111.87632653061226), Caption(word='is', start=111.87632653061226, end=112.13632653061225), Caption(word='applied', start=112.13632653061225, end=112.41632653061225), Caption(word='to', start=112.41632653061225, end=112.65632653061226), Caption(word='ensure', start=112.65632653061226, end=112.85632653061225), Caption(word='these', start=112.85632653061225, end=113.17632653061226), Caption(word='embeddings', start=113.17632653061226, end=113.75632653061226), Caption(word='are', start=113.75632653061226, end=113.85632653061225), Caption(word='well', start=113.85632653061225, end=114.03632653061226), Caption(word='aligned.', start=114.03632653061226, end=114.49632653061227)], start=104.17632653061226, end=115.06938775510206),\n",
       " Headline(content='Aligned Shape Latent Diffusion Model (ASLDM)', start=115.06938775510206, end=132.2318367346939),\n",
       " Text(content='ASLDM builds on the success of Latent Diffusion Models. It maps 2D images and texts to the shape latent embeddings within the aligned space. This model leverages a UNet-like architecture to generate consistent and high-quality 3D shapes.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d540>, audio_path='./audio/text_13.wav', captions=[Caption(word='ASLDM', start=115.06938775510206, end=115.84938775510206), Caption(word='builds', start=115.84938775510206, end=116.06938775510206), Caption(word='on', start=116.06938775510206, end=116.34938775510206), Caption(word='the', start=116.34938775510206, end=116.42938775510206), Caption(word='success', start=116.42938775510206, end=116.78938775510206), Caption(word='of', start=116.78938775510206, end=117.14938775510205), Caption(word='latent', start=117.14938775510205, end=117.44938775510205), Caption(word='diffusion', start=117.44938775510205, end=117.84938775510206), Caption(word='models.', start=117.84938775510206, end=118.48938775510206), Caption(word='It', start=119.22938775510205, end=119.26938775510206), Caption(word='maps', start=119.26938775510206, end=119.50938775510205), Caption(word='2D', start=119.50938775510205, end=120.20938775510206), Caption(word='images', start=120.20938775510206, end=120.58938775510205), Caption(word='and', start=120.58938775510205, end=120.86938775510205), Caption(word='texts', start=120.86938775510205, end=121.12938775510206), Caption(word='to', start=121.12938775510206, end=121.80938775510205), Caption(word='the', start=121.80938775510205, end=121.92938775510206), Caption(word='shape', start=121.92938775510206, end=122.24938775510205), Caption(word='latent', start=122.24938775510205, end=122.54938775510206), Caption(word='embeddings', start=122.54938775510206, end=123.20938775510206), Caption(word='within', start=123.20938775510206, end=123.46938775510206), Caption(word='the', start=123.46938775510206, end=123.66938775510205), Caption(word='aligned', start=123.66938775510205, end=123.86938775510205), Caption(word='space.', start=123.86938775510205, end=124.32938775510206), Caption(word='This', start=125.18938775510206, end=125.26938775510206), Caption(word='model', start=125.26938775510206, end=125.58938775510205), Caption(word='leverages', start=125.58938775510205, end=126.24938775510205), Caption(word='a', start=126.24938775510205, end=126.54938775510206), Caption(word='unit', start=126.54938775510206, end=126.88938775510206), Caption(word='-like', start=126.88938775510206, end=127.30938775510205), Caption(word='architecture', start=127.30938775510205, end=127.72938775510205), Caption(word='to', start=128.21938775510205, end=128.58938775510205), Caption(word='generate', start=128.58938775510205, end=129.00938775510207), Caption(word='consistent', start=129.00938775510207, end=129.46938775510205), Caption(word='and', start=129.46938775510205, end=129.92938775510206), Caption(word='high', start=129.92938775510206, end=130.18938775510205), Caption(word='-quality', start=130.18938775510205, end=130.56938775510207), Caption(word='3D', start=130.56938775510207, end=131.28938775510204), Caption(word='shapes.', start=131.28938775510204, end=131.70938775510206)], start=115.06938775510206, end=132.2318367346939),\n",
       " Equation(content='\\\\mathcal{L} = \\\\| \\\\mathbf{E}_s^{(0)} - \\\\mathbf{E}_s^{(t)} \\\\|', start=132.2318367346939, end=139.15428571428575),\n",
       " Text(content='This objective function ensures the generative process maintains high fidelity throughout the diffusion steps.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d3c0>, audio_path='./audio/text_15.wav', captions=[Caption(word='This', start=132.2318367346939, end=132.4118367346939), Caption(word='objective', start=132.4118367346939, end=132.6318367346939), Caption(word='function', start=132.6318367346939, end=133.2918367346939), Caption(word='ensures', start=133.2918367346939, end=133.6518367346939), Caption(word='the', start=133.6518367346939, end=134.0118367346939), Caption(word='generative', start=134.0118367346939, end=134.4718367346939), Caption(word='process', start=134.4718367346939, end=134.9918367346939), Caption(word='maintains', start=134.9918367346939, end=135.6718367346939), Caption(word='high', start=135.6718367346939, end=136.2918367346939), Caption(word='fidelity', start=136.2918367346939, end=136.8118367346939), Caption(word='throughout', start=136.8118367346939, end=137.3118367346939), Caption(word='the', start=137.3118367346939, end=137.6318367346939), Caption(word='diffusion', start=137.6318367346939, end=137.9918367346939), Caption(word='steps.', start=137.9918367346939, end=138.5518367346939)], start=132.2318367346939, end=139.15428571428575),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x3.png', start=139.15428571428575, end=156.83918367346942),\n",
       " Text(content='This figure illustrates the text-conditioned 3D shape generation capability of Michelangelo. Even abstract or detailed textual descriptions result in smooth, high-fidelity 3D shapes, proving the model’s efficiency in capturing and translating semantic details.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d240>, audio_path='./audio/text_17.wav', captions=[Caption(word='This', start=139.15428571428575, end=139.35428571428574), Caption(word='figure', start=139.35428571428574, end=139.73428571428576), Caption(word='illustrates', start=139.73428571428576, end=140.17428571428576), Caption(word='the', start=140.17428571428576, end=140.59428571428575), Caption(word='text', start=140.59428571428575, end=140.77428571428575), Caption(word='condition', start=140.77428571428575, end=141.23428571428576), Caption(word='3D', start=141.23428571428576, end=142.03428571428574), Caption(word='shape', start=142.03428571428574, end=142.37428571428575), Caption(word='generation', start=142.37428571428575, end=142.79428571428573), Caption(word='capability', start=142.79428571428573, end=143.59428571428575), Caption(word='of', start=143.59428571428575, end=144.01428571428576), Caption(word='Michelangelo.', start=144.01428571428576, end=144.69428571428574), Caption(word='Even', start=145.49428571428575, end=145.59428571428575), Caption(word='abstract', start=145.59428571428575, end=145.97428571428574), Caption(word='or', start=145.97428571428574, end=146.37428571428575), Caption(word='detailed', start=146.37428571428575, end=146.71428571428575), Caption(word='textual', start=146.71428571428575, end=147.29428571428576), Caption(word='descriptions', start=147.29428571428576, end=147.85428571428574), Caption(word='result', start=147.85428571428574, end=148.59428571428575), Caption(word='in', start=148.59428571428575, end=148.83428571428576), Caption(word='smooth,', start=148.83428571428576, end=149.19428571428574), Caption(word='high', start=149.19428571428574, end=149.63428571428574), Caption(word='-fidelity', start=149.63428571428574, end=150.03428571428574), Caption(word='3D', start=150.03428571428574, end=150.89428571428576), Caption(word='shapes,', start=150.89428571428576, end=151.29428571428576), Caption(word='proving', start=151.77428571428575, end=152.23428571428576), Caption(word='the', start=152.23428571428576, end=152.47428571428574), Caption(word=\"model's\", start=152.47428571428574, end=152.89428571428576), Caption(word='efficiency', start=152.89428571428576, end=153.13428571428574), Caption(word='in', start=153.13428571428574, end=153.75428571428574), Caption(word='capturing', start=153.75428571428574, end=154.21428571428575), Caption(word='and', start=154.21428571428575, end=154.53428571428574), Caption(word='translating', start=154.53428571428574, end=155.09428571428575), Caption(word='semantic', start=155.09428571428575, end=155.45428571428576), Caption(word='details.', start=155.45428571428576, end=156.11428571428576)], start=139.15428571428575, end=156.83918367346942),\n",
       " Headline(content='Experimental Validation', start=156.83918367346942, end=177.34530612244902),\n",
       " Text(content='Extensive experiments were conducted using the ShapeNet dataset and a customized 3D Cartoon Monster dataset. Metrics like Intersection Over Union (IoU) and newly proposed Shape-Image Score (SI-S) and Shape-Text Score (ST-S) were used to validate the model’s performance.', audio=<generator object TextToSpeechClient.convert at 0x2bc42d0c0>, audio_path='./audio/text_19.wav', captions=[Caption(word='Extensive', start=156.83918367346942, end=157.27918367346942), Caption(word='experiments', start=157.27918367346942, end=157.83918367346942), Caption(word='were', start=157.83918367346942, end=158.11918367346942), Caption(word='conducted', start=158.11918367346942, end=158.45918367346943), Caption(word='using', start=158.45918367346943, end=158.89918367346942), Caption(word='the', start=158.89918367346942, end=159.07918367346943), Caption(word='ShapeNet', start=159.07918367346943, end=159.57918367346943), Caption(word='dataset', start=159.57918367346943, end=159.95918367346943), Caption(word='and', start=159.95918367346943, end=160.79918367346943), Caption(word='a', start=160.79918367346943, end=160.8791836734694), Caption(word='customized', start=160.8791836734694, end=161.21918367346942), Caption(word='3D', start=161.21918367346942, end=162.27918367346942), Caption(word='cartoon', start=162.27918367346942, end=162.67918367346942), Caption(word='monster', start=162.67918367346942, end=163.23918367346943), Caption(word='dataset.', start=163.23918367346943, end=164.01918367346943), Caption(word='Metrics', start=164.51918367346943, end=164.91918367346943), Caption(word='like', start=164.91918367346943, end=165.21918367346942), Caption(word='Intersection', start=165.21918367346942, end=165.8791836734694), Caption(word='Over', start=165.8791836734694, end=166.17918367346942), Caption(word='Union,', start=166.17918367346942, end=166.57918367346943), Caption(word='IU,', start=167.3791836734694, end=167.5391836734694), Caption(word='and', start=168.2591836734694, end=168.51918367346943), Caption(word='newly', start=168.51918367346943, end=168.73918367346943), Caption(word='proposed', start=168.73918367346943, end=169.2591836734694), Caption(word='Shape', start=169.2591836734694, end=169.71918367346942), Caption(word='Image', start=169.71918367346942, end=170.0391836734694), Caption(word='Score,', start=170.0391836734694, end=170.41918367346943), Caption(word='SIS,', start=171.0391836734694, end=171.43918367346942), Caption(word='and', start=172.23918367346943, end=172.23918367346943), Caption(word='Shape', start=172.23918367346943, end=172.47918367346944), Caption(word='Text', start=172.47918367346944, end=172.73918367346943), Caption(word='Score,', start=172.73918367346943, end=173.1991836734694), Caption(word='STS,', start=173.7591836734694, end=174.29918367346943), Caption(word='were', start=174.8191836734694, end=174.91918367346943), Caption(word='used', start=174.91918367346943, end=175.13918367346943), Caption(word='to', start=175.13918367346943, end=175.3191836734694), Caption(word='validate', start=175.3191836734694, end=175.6591836734694), Caption(word='the', start=175.6591836734694, end=175.99918367346942), Caption(word=\"model's\", start=175.99918367346942, end=176.35918367346943), Caption(word='performance.', start=176.35918367346943, end=176.79918367346943)], start=156.83918367346942, end=177.34530612244902),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x4.png', start=177.34530612244902, end=188.9436734693878),\n",
       " Text(content='Here’s an ablation study showcasing the importance of training the model in the aligned space. Training in this manner leads to samples that are significantly closer to the given text and image conditions.', audio=<generator object TextToSpeechClient.convert at 0x2bc42cf40>, audio_path='./audio/text_21.wav', captions=[Caption(word=\"Here's\", start=177.34530612244902, end=177.64530612244903), Caption(word='an', start=177.64530612244903, end=177.74530612244902), Caption(word='ablation', start=177.74530612244902, end=178.045306122449), Caption(word='study', start=178.045306122449, end=178.52530612244902), Caption(word='showcasing', start=178.52530612244902, end=179.005306122449), Caption(word='the', start=179.005306122449, end=179.385306122449), Caption(word='importance', start=179.385306122449, end=179.68530612244902), Caption(word='of', start=179.68530612244902, end=179.985306122449), Caption(word='training', start=179.985306122449, end=180.30530612244902), Caption(word='the', start=180.30530612244902, end=180.605306122449), Caption(word='model', start=180.605306122449, end=180.86530612244903), Caption(word='in', start=180.86530612244903, end=181.14530612244903), Caption(word='the', start=181.14530612244903, end=181.24530612244902), Caption(word='aligned', start=181.24530612244902, end=181.42530612244903), Caption(word='space.', start=181.42530612244903, end=182.06530612244902), Caption(word='Training', start=182.70530612244903, end=182.985306122449), Caption(word='in', start=182.985306122449, end=183.265306122449), Caption(word='this', start=183.265306122449, end=183.40530612244902), Caption(word='manner', start=183.40530612244902, end=183.70530612244903), Caption(word='leads', start=183.70530612244903, end=184.045306122449), Caption(word='to', start=184.045306122449, end=184.30530612244902), Caption(word='samples', start=184.30530612244902, end=184.725306122449), Caption(word='that', start=184.725306122449, end=185.02530612244902), Caption(word='are', start=185.02530612244902, end=185.12530612244902), Caption(word='significantly', start=185.12530612244902, end=185.545306122449), Caption(word='closer', start=185.545306122449, end=186.18530612244902), Caption(word='to', start=186.18530612244902, end=186.58530612244903), Caption(word='the', start=186.58530612244903, end=186.68530612244902), Caption(word='given', start=186.68530612244902, end=186.92530612244903), Caption(word='text', start=186.92530612244903, end=187.30530612244902), Caption(word='and', start=187.30530612244902, end=187.545306122449), Caption(word='image', start=187.545306122449, end=187.80530612244902), Caption(word='conditions.', start=187.80530612244902, end=188.265306122449)], start=177.34530612244902, end=188.9436734693878),\n",
       " Headline(content='Conclusion and Future Work', start=None, end=None),\n",
       " Text(content='Michelangelo has proven effective in aligning and generating 3D shapes from 2D images and texts. The results show higher quality and diversity of 3D shape generation compared to other methods. Future work could address the need for ground truth 3D shapes for training, possibly leveraging differentiable rendering from multi-view images.', audio=<generator object TextToSpeechClient.convert at 0x2bc42cdc0>, audio_path='./audio/text_23.wav', captions=[Caption(word='Michelangelo', start=188.9436734693878, end=189.5236734693878), Caption(word='has', start=189.5236734693878, end=189.82367346938778), Caption(word='proven', start=189.82367346938778, end=190.0236734693878), Caption(word='effective', start=190.0236734693878, end=190.5036734693878), Caption(word='in', start=190.5036734693878, end=190.8836734693878), Caption(word='aligning', start=190.8836734693878, end=191.42367346938778), Caption(word='and', start=191.42367346938778, end=191.5636734693878), Caption(word='generating', start=191.5636734693878, end=192.04367346938778), Caption(word='3D', start=192.04367346938778, end=192.8436734693878), Caption(word='shapes', start=192.8436734693878, end=193.1836734693878), Caption(word='from', start=193.1836734693878, end=193.58367346938778), Caption(word='2D', start=193.58367346938778, end=194.26367346938778), Caption(word='images', start=194.26367346938778, end=194.6836734693878), Caption(word='and', start=194.6836734693878, end=195.04367346938778), Caption(word='texts.', start=195.04367346938778, end=195.3436734693878), Caption(word='The', start=196.2236734693878, end=196.2436734693878), Caption(word='results', start=196.2436734693878, end=196.54367346938778), Caption(word='show', start=196.54367346938778, end=196.9636734693878), Caption(word='higher', start=196.9636734693878, end=197.5236734693878), Caption(word='quality', start=197.5236734693878, end=198.1036734693878), Caption(word='and', start=198.1036734693878, end=198.80367346938777), Caption(word='diversity', start=198.80367346938777, end=199.30367346938777), Caption(word='of', start=199.30367346938777, end=199.7836734693878), Caption(word='3D', start=199.7836734693878, end=200.42367346938778), Caption(word='shape', start=200.42367346938778, end=200.7436734693878), Caption(word='generation', start=200.7436734693878, end=201.1636734693878), Caption(word='compared', start=201.1636734693878, end=201.7836734693878), Caption(word='to', start=201.7836734693878, end=202.1236734693878), Caption(word='other', start=202.1236734693878, end=202.32367346938778), Caption(word='methods.', start=202.32367346938778, end=202.7436734693878), Caption(word='Future', start=203.6036734693878, end=203.6236734693878), Caption(word='work', start=203.6236734693878, end=204.0836734693878), Caption(word='could', start=204.0836734693878, end=204.32367346938778), Caption(word='address', start=204.32367346938778, end=204.5636734693878), Caption(word='the', start=204.5636734693878, end=204.8436734693878), Caption(word='need', start=204.8436734693878, end=205.04367346938778), Caption(word='for', start=205.04367346938778, end=205.2836734693878), Caption(word='ground', start=205.2836734693878, end=205.6636734693878), Caption(word='truth,', start=205.6636734693878, end=206.0036734693878), Caption(word='3D', start=206.32367346938778, end=206.7236734693878), Caption(word='shapes', start=206.7236734693878, end=207.1036734693878), Caption(word='for', start=207.1036734693878, end=207.4636734693878), Caption(word='training,', start=207.4636734693878, end=207.80367346938777), Caption(word='possibly', start=208.3436734693878, end=208.6836734693878), Caption(word='leveraging', start=208.6836734693878, end=209.2236734693878), Caption(word='differentiable', start=209.2236734693878, end=210.0636734693878), Caption(word='rendering', start=210.0636734693878, end=210.4036734693878), Caption(word='from', start=210.4036734693878, end=210.9036734693878), Caption(word='multi', start=210.9036734693878, end=211.1836734693878), Caption(word='-view', start=211.1836734693878, end=211.6036734693878), Caption(word='images.', start=211.6036734693878, end=212.0836734693878)], start=188.9436734693878, end=212.71510204081636),\n",
       " Text(content=\"Thanks for watching Arxflix! Don't forget to like and subscribe for more deep dives into the latest research papers. Leave a comment below on what paper you would like us to cover next!\", audio=<generator object TextToSpeechClient.convert at 0x2bc42c940>, audio_path=None, captions=None, start=None, end=None)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_contents = parse_script(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevenlabs_client = ElevenLabs(\n",
    "  api_key=ELEVENLABS_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_caption(result: dict) -> list[Caption]:\n",
    "    captions: list[Caption] = []\n",
    "    for segment in result['segments']:\n",
    "        for word in segment['words']: # type: ignore\n",
    "            _word = word['word'] # type: ignore\n",
    "            # Remove leading space if there is one\n",
    "            if _word.startswith(' '):\n",
    "                _word = _word[1:]\n",
    "            caption = Caption(word=_word, start=word['start'], end=word['end']) # type: ignore\n",
    "            captions.append(caption)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_and_caption(script_contents: list[RichContent | Text]) -> list[RichContent | Text]:\n",
    "  for i, script_content in enumerate(script_contents):\n",
    "    match script_content:\n",
    "      case RichContent(content=content):\n",
    "          pass\n",
    "      case Text(content=content, audio=None, captions=None):\n",
    "          # script_content.audio = elevenlabs_client.generate(\n",
    "          #     text=content,\n",
    "          #     voice=ELEVENLABS_VOICE,\n",
    "          #     model=ELEVENLABS_MODEL\n",
    "          # )\n",
    "          audio_path = f'./audio/text_{i}.wav'\n",
    "          # save(script_content.audio, audio_path)\n",
    "          audio, sr = torchaudio.load(audio_path)\n",
    "          model = whisper.load_model('base.en')\n",
    "          option = whisper.DecodingOptions(language='en', fp16=True, without_timestamps=False, task='transcribe')\n",
    "          result = model.transcribe(f'./audio/text_{i}.wav', word_timestamps=True)\n",
    "          script_content.captions = make_caption(result)\n",
    "          script_content.audio_path = audio_path\n",
    "          total_audio_duration = audio.size(1) / sr\n",
    "          script_content.end = total_audio_duration\n",
    "  return script_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "script_contents = generate_audio_and_caption(script_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_caption_offset_and_gap(script_contents: list[RichContent | Text]) -> list[RichContent | Text]:\n",
    "    offset = 0\n",
    "    for i, script_content in enumerate(script_contents):\n",
    "        if not(isinstance(script_content, Text)):\n",
    "            continue\n",
    "        if not script_content.captions:\n",
    "            continue\n",
    "        for caption in script_content.captions:\n",
    "            caption.start += offset\n",
    "            caption.end += offset\n",
    "        script_content.start = offset\n",
    "        if script_content.end:\n",
    "            script_content.end = script_content.end + offset\n",
    "        else:\n",
    "            script_content.end = script_content.captions[-1].end\n",
    "        offset = script_content.end\n",
    "    return script_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_contents = add_caption_offset_and_gap(script_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_rich_content_time(script_contents: list[RichContent | Text]) -> list[RichContent | Text]:\n",
    "    k = 0\n",
    "    while k < len(script_contents):\n",
    "        current_rich_content_group = []\n",
    "        while k < len(script_contents) and not isinstance(script_contents[k], Text):\n",
    "            current_rich_content_group.append(script_contents[k])\n",
    "            k += 1\n",
    "        \n",
    "        if k >= len(script_contents):\n",
    "            break\n",
    "\n",
    "        next_text_group = []\n",
    "        while k < len(script_contents) and isinstance(script_contents[k], Text):\n",
    "            next_text_group.append(script_contents[k])\n",
    "            k += 1\n",
    "        \n",
    "        if not next_text_group:\n",
    "            break\n",
    "\n",
    "        total_duration = next_text_group[-1].end - next_text_group[0].start\n",
    "        duration_per_rich_content = total_duration / len(current_rich_content_group)\n",
    "        offset = next_text_group[0].start\n",
    "        for i, rich_content in enumerate(current_rich_content_group):\n",
    "            rich_content.start = offset + i * duration_per_rich_content\n",
    "            rich_content.end = offset + (i + 1) * duration_per_rich_content\n",
    "            # print(f\"Asigning {rich_content.start} - {rich_content.end} to {rich_content}\")\n",
    "    return script_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_contents = fill_rich_content_time(script_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Headline(content='Michelangelo: Conditional 3D Shape Generation', start=0.0, end=24.372244897959185),\n",
       " Text(content='Welcome back to Arxflix! Today, we’re diving into an intriguing new paper titled \"Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation\". This research looks at a novel approach for generating 3D shapes conditioned on 2D images or texts, which is a significant breakthrough in the field of deep learning and 3D modeling.', audio=None, audio_path='./audio/text_1.wav', captions=[Caption(word='Welcome', start=0.0, end=0.2), Caption(word='back', start=0.2, end=0.56), Caption(word='to', start=0.56, end=0.76), Caption(word='ARXFlicks.', start=0.76, end=1.3), Caption(word='Today', start=2.0, end=2.18), Caption(word=\"we're\", start=2.18, end=2.44), Caption(word='diving', start=2.44, end=2.6), Caption(word='into', start=2.6, end=3.04), Caption(word='an', start=3.04, end=3.14), Caption(word='intriguing', start=3.14, end=3.46), Caption(word='new', start=3.46, end=3.82), Caption(word='paper', start=3.82, end=4.16), Caption(word='titled', start=4.16, end=4.6), Caption(word='Michelangelo,', start=4.6, end=5.58), Caption(word='Conditional', start=6.18, end=6.64), Caption(word='3D', start=6.64, end=7.26), Caption(word='Shape', start=7.26, end=7.56), Caption(word='Generation', start=7.56, end=8.0), Caption(word='based', start=8.0, end=8.6), Caption(word='on', start=8.6, end=8.92), Caption(word='shape', start=8.92, end=9.32), Caption(word='image', start=9.32, end=9.74), Caption(word='text', start=9.74, end=10.2), Caption(word='aligned', start=10.2, end=10.64), Caption(word='latent', start=10.64, end=11.18), Caption(word='representation.', start=11.18, end=11.88), Caption(word='This', start=12.88, end=13.02), Caption(word='research', start=13.02, end=13.44), Caption(word='looks', start=13.44, end=13.7), Caption(word='at', start=13.7, end=13.88), Caption(word='a', start=13.88, end=13.94), Caption(word='novel', start=13.94, end=14.14), Caption(word='approach', start=14.14, end=14.72), Caption(word='for', start=14.72, end=14.94), Caption(word='generating', start=14.94, end=15.24), Caption(word='3D', start=15.24, end=16.18), Caption(word='shapes', start=16.18, end=16.56), Caption(word='conditioned', start=16.56, end=16.92), Caption(word='on', start=16.92, end=17.36), Caption(word='2D', start=17.36, end=17.9), Caption(word='images', start=17.9, end=18.28), Caption(word='or', start=18.28, end=18.6), Caption(word='texts,', start=18.6, end=18.86), Caption(word='which', start=19.52, end=19.74), Caption(word='is', start=19.74, end=19.86), Caption(word='a', start=19.86, end=19.98), Caption(word='significant', start=19.98, end=20.3), Caption(word='breakthrough', start=20.3, end=20.74), Caption(word='in', start=20.74, end=21.16), Caption(word='the', start=21.16, end=21.24), Caption(word='field', start=21.24, end=21.46), Caption(word='of', start=21.46, end=21.62), Caption(word='deep', start=21.62, end=21.78), Caption(word='learning', start=21.78, end=22.12), Caption(word='and', start=22.12, end=22.74), Caption(word='3D', start=22.74, end=23.28), Caption(word='modeling.', start=23.28, end=23.66)], start=0, end=24.372244897959185),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x1.png', start=24.372244897959185, end=47.25551020408163),\n",
       " Text(content='Here’s an overview of the Michelangelo pipeline showing the two key models: the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) and the Aligned Shape Latent Diffusion Model (ASLDM). These models work together to bridge the distribution gap between 3D shapes and 2D images or texts.', audio=None, audio_path='./audio/text_3.wav', captions=[Caption(word=\"Here's\", start=24.372244897959185, end=24.672244897959185), Caption(word='an', start=24.672244897959185, end=24.772244897959183), Caption(word='overview', start=24.772244897959183, end=25.172244897959185), Caption(word='of', start=25.172244897959185, end=25.552244897959184), Caption(word='the', start=25.552244897959184, end=25.672244897959185), Caption(word='Michelangelo', start=25.672244897959185, end=26.412244897959184), Caption(word='pipeline', start=26.412244897959184, end=26.792244897959186), Caption(word='showing', start=26.792244897959186, end=27.892244897959184), Caption(word='the', start=27.892244897959184, end=28.152244897959186), Caption(word='two', start=28.152244897959186, end=28.392244897959184), Caption(word='key', start=28.392244897959184, end=28.752244897959184), Caption(word='models.', start=28.752244897959184, end=29.192244897959185), Caption(word='The', start=30.112244897959187, end=30.132244897959183), Caption(word='shape', start=30.132244897959183, end=30.412244897959184), Caption(word='image', start=30.412244897959184, end=30.832244897959185), Caption(word='text', start=30.832244897959185, end=31.252244897959184), Caption(word='aligned', start=31.252244897959184, end=31.732244897959184), Caption(word='variational', start=31.732244897959184, end=32.81224489795918), Caption(word='auto', start=32.81224489795918, end=33.21224489795918), Caption(word='encoder,', start=33.21224489795918, end=33.81224489795918), Caption(word='CITA', start=34.152244897959186, end=34.512244897959185), Caption(word='-VAT', start=34.512244897959185, end=34.792244897959186), Caption(word='-E,', start=34.792244897959186, end=35.052244897959184), Caption(word='and', start=35.652244897959186, end=35.75224489795919), Caption(word='the', start=35.75224489795919, end=35.872244897959185), Caption(word='aligned', start=35.872244897959185, end=36.292244897959186), Caption(word='shape', start=36.292244897959186, end=36.89224489795919), Caption(word='latent', start=36.89224489795919, end=37.332244897959185), Caption(word='diffusion', start=37.332244897959185, end=37.71224489795918), Caption(word='model,', start=37.71224489795918, end=38.43224489795919), Caption(word='ASLDM.', start=38.832244897959185, end=39.89224489795919), Caption(word='These', start=40.63224489795918, end=40.71224489795918), Caption(word='models', start=40.71224489795918, end=41.07224489795918), Caption(word='work', start=41.07224489795918, end=41.39224489795919), Caption(word='together', start=41.39224489795919, end=41.71224489795918), Caption(word='to', start=41.71224489795918, end=42.052244897959184), Caption(word='bridge', start=42.052244897959184, end=42.232244897959184), Caption(word='the', start=42.232244897959184, end=42.512244897959185), Caption(word='distribution', start=42.512244897959185, end=42.95224489795918), Caption(word='gap', start=42.95224489795918, end=43.372244897959185), Caption(word='between', start=43.372244897959185, end=43.77224489795918), Caption(word='3D', start=43.77224489795918, end=44.39224489795919), Caption(word='shapes', start=44.39224489795919, end=44.732244897959184), Caption(word='and', start=44.732244897959184, end=45.07224489795918), Caption(word='2D', start=45.07224489795918, end=45.59224489795918), Caption(word='images', start=45.59224489795918, end=45.95224489795918), Caption(word='or', start=45.95224489795918, end=46.292244897959186), Caption(word='texts.', start=46.292244897959186, end=46.53224489795919)], start=24.372244897959185, end=47.25551020408163),\n",
       " Headline(content='The Challenge', start=47.25551020408163, end=66.66448979591837),\n",
       " Text(content='The main challenge addressed in this paper is the significant distribution gap between 3D shapes and their 2D or textual descriptions. Directly mapping these modalities can lead to inconsistent and poor-quality 3D shapes. Michelangelo tackles this through the concept of alignment before generation.', audio=None, audio_path='./audio/text_5.wav', captions=[Caption(word='The', start=47.25551020408163, end=47.41551020408163), Caption(word='main', start=47.41551020408163, end=47.57551020408163), Caption(word='challenge', start=47.57551020408163, end=47.97551020408163), Caption(word='addressed', start=47.97551020408163, end=48.275510204081634), Caption(word='in', start=48.275510204081634, end=48.57551020408163), Caption(word='this', start=48.57551020408163, end=48.67551020408163), Caption(word='paper', start=48.67551020408163, end=49.03551020408163), Caption(word='is', start=49.03551020408163, end=49.275510204081634), Caption(word='the', start=49.275510204081634, end=49.41551020408163), Caption(word='significant', start=49.41551020408163, end=49.75551020408163), Caption(word='distribution', start=49.75551020408163, end=50.37551020408163), Caption(word='gap', start=50.37551020408163, end=50.955510204081634), Caption(word='between', start=50.955510204081634, end=51.33551020408163), Caption(word='3D', start=51.33551020408163, end=51.99551020408163), Caption(word='shapes', start=51.99551020408163, end=52.29551020408163), Caption(word='and', start=52.29551020408163, end=52.735510204081635), Caption(word='their', start=52.735510204081635, end=52.87551020408163), Caption(word='2D', start=52.87551020408163, end=53.53551020408163), Caption(word='or', start=53.53551020408163, end=54.05551020408163), Caption(word='textual', start=54.05551020408163, end=54.35551020408163), Caption(word='descriptions.', start=54.35551020408163, end=54.83551020408163), Caption(word='Directly', start=55.93551020408163, end=56.33551020408163), Caption(word='mapping', start=56.33551020408163, end=56.49551020408163), Caption(word='these', start=56.49551020408163, end=56.87551020408163), Caption(word='modalities', start=56.87551020408163, end=57.27551020408163), Caption(word='can', start=57.27551020408163, end=57.79551020408163), Caption(word='lead', start=57.79551020408163, end=58.01551020408163), Caption(word='to', start=58.01551020408163, end=58.17551020408163), Caption(word='inconsistent', start=58.17551020408163, end=58.71551020408163), Caption(word='and', start=58.71551020408163, end=59.15551020408163), Caption(word='poor', start=59.15551020408163, end=59.39551020408163), Caption(word='quality', start=59.39551020408163, end=59.735510204081635), Caption(word='3D', start=59.735510204081635, end=60.53551020408163), Caption(word='shapes.', start=60.53551020408163, end=61.03551020408163), Caption(word='Michelangelo', start=62.01551020408163, end=62.455510204081634), Caption(word='tackles', start=62.455510204081634, end=62.89551020408163), Caption(word='this', start=62.89551020408163, end=63.375510204081635), Caption(word='through', start=63.375510204081635, end=63.57551020408163), Caption(word='the', start=63.57551020408163, end=63.75551020408163), Caption(word='concept', start=63.75551020408163, end=64.15551020408162), Caption(word='of', start=64.15551020408162, end=64.45551020408163), Caption(word='alignment', start=64.45551020408163, end=64.73551020408163), Caption(word='before', start=64.73551020408163, end=65.35551020408164), Caption(word='generation.', start=65.35551020408164, end=65.85551020408164)], start=47.25551020408163, end=66.66448979591837),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x2.png', start=66.66448979591837, end=82.49469387755103),\n",
       " Text(content='Here’s a comparison showcasing how Michelangelo maintains detailed 3D shape generation compared to other methods. Notice the smooth surfaces and detailed structure, which contrast with the oversimplified and noisy results from other models.', audio=None, audio_path='./audio/text_7.wav', captions=[Caption(word=\"Here's\", start=66.66448979591837, end=67.00448979591837), Caption(word='a', start=67.00448979591837, end=67.08448979591837), Caption(word='comparison', start=67.08448979591837, end=67.58448979591837), Caption(word='showcasing', start=67.58448979591837, end=68.06448979591838), Caption(word='how', start=68.06448979591838, end=68.54448979591837), Caption(word='Michelangelo', start=68.54448979591837, end=69.22448979591837), Caption(word='maintains', start=69.22448979591837, end=69.62448979591836), Caption(word='detailed', start=69.62448979591836, end=70.26448979591837), Caption(word='3D', start=70.26448979591837, end=71.20448979591838), Caption(word='-shaped', start=71.20448979591838, end=71.50448979591837), Caption(word='generation', start=71.50448979591837, end=72.10448979591837), Caption(word='compared', start=72.10448979591837, end=72.86448979591837), Caption(word='to', start=72.86448979591837, end=73.16448979591837), Caption(word='other', start=73.16448979591837, end=73.36448979591837), Caption(word='methods.', start=73.36448979591837, end=73.76448979591837), Caption(word='Notice', start=74.36448979591837, end=74.60448979591837), Caption(word='the', start=74.60448979591837, end=74.90448979591837), Caption(word='smooth', start=74.90448979591837, end=75.18448979591837), Caption(word='surfaces', start=75.18448979591837, end=75.86448979591837), Caption(word='and', start=75.86448979591837, end=76.22448979591837), Caption(word='detailed', start=76.22448979591837, end=76.48448979591836), Caption(word='structure,', start=76.48448979591836, end=77.10448979591837), Caption(word='which', start=77.52448979591837, end=77.82448979591837), Caption(word='contrast', start=77.82448979591837, end=78.22448979591837), Caption(word='with', start=78.22448979591837, end=78.74448979591837), Caption(word='the', start=78.74448979591837, end=78.86448979591837), Caption(word='oversimplified', start=78.86448979591837, end=79.84448979591838), Caption(word='and', start=79.84448979591838, end=80.04448979591837), Caption(word='noisy', start=80.04448979591837, end=80.32448979591837), Caption(word='results', start=80.32448979591837, end=80.84448979591838), Caption(word='from', start=80.84448979591838, end=81.20448979591836), Caption(word='other', start=81.20448979591836, end=81.46448979591837), Caption(word='models.', start=81.46448979591837, end=81.84448979591838)], start=66.66448979591837, end=82.49469387755103),\n",
       " Headline(content='Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)', start=82.49469387755103, end=104.17632653061226),\n",
       " Text(content='SITA-VAE creates a shared latent space for 3D shapes, images, and texts. It uses a CLIP-based encoder to align images and texts and a trainable 3D shape encoder to map 3D shapes into this shared space. The transformer-based neural field decoder then ensures high-quality 3D reconstruction.', audio=None, audio_path='./audio/text_9.wav', captions=[Caption(word='CEDA', start=82.49469387755103, end=82.85469387755103), Caption(word='-Vay', start=82.85469387755103, end=83.09469387755102), Caption(word='creates', start=83.09469387755102, end=83.47469387755103), Caption(word='a', start=83.47469387755103, end=83.63469387755103), Caption(word='shared', start=83.63469387755103, end=83.95469387755102), Caption(word='latent', start=83.95469387755102, end=84.17469387755104), Caption(word='space', start=84.17469387755104, end=84.57469387755103), Caption(word='for', start=84.57469387755103, end=84.89469387755103), Caption(word='3D', start=84.89469387755103, end=85.53469387755104), Caption(word='shapes,', start=85.53469387755104, end=85.87469387755102), Caption(word='images,', start=86.19469387755103, end=86.31469387755102), Caption(word='and', start=86.31469387755102, end=86.79469387755103), Caption(word='texts.', start=86.79469387755103, end=87.03469387755104), Caption(word='It', start=87.93469387755103, end=87.99469387755103), Caption(word='uses', start=87.99469387755103, end=88.27469387755103), Caption(word='a', start=88.27469387755103, end=88.47469387755103), Caption(word='clip', start=88.47469387755103, end=88.65469387755103), Caption(word='-based', start=88.65469387755103, end=89.03469387755104), Caption(word='encoder', start=89.03469387755104, end=89.47469387755103), Caption(word='to', start=89.47469387755103, end=90.17469387755102), Caption(word='align', start=90.17469387755102, end=90.37469387755102), Caption(word='images', start=90.37469387755102, end=90.87469387755102), Caption(word='and', start=90.87469387755102, end=91.19469387755103), Caption(word='texts', start=91.19469387755103, end=91.49469387755103), Caption(word='and', start=91.49469387755103, end=92.17469387755102), Caption(word='a', start=92.17469387755102, end=92.25469387755103), Caption(word='trainable', start=92.25469387755103, end=92.67469387755102), Caption(word='3D', start=92.67469387755102, end=93.35469387755103), Caption(word='shape', start=93.35469387755103, end=93.65469387755103), Caption(word='encoder', start=93.65469387755103, end=94.21469387755103), Caption(word='to', start=94.21469387755103, end=94.77469387755103), Caption(word='map', start=94.77469387755103, end=94.93469387755103), Caption(word='3D', start=94.93469387755103, end=95.53469387755104), Caption(word='shapes', start=95.53469387755104, end=95.87469387755102), Caption(word='into', start=95.87469387755102, end=96.29469387755103), Caption(word='this', start=96.29469387755103, end=96.55469387755103), Caption(word='shared', start=96.55469387755103, end=97.07469387755103), Caption(word='space.', start=97.07469387755103, end=97.59469387755102), Caption(word='The', start=98.41469387755103, end=98.45469387755102), Caption(word='transformer', start=98.45469387755102, end=98.89469387755102), Caption(word='-based', start=98.89469387755102, end=99.39469387755102), Caption(word='neural', start=99.39469387755102, end=99.71469387755103), Caption(word='field', start=99.71469387755103, end=100.17469387755102), Caption(word='decoder', start=100.17469387755102, end=100.65469387755103), Caption(word='then', start=100.65469387755103, end=101.01469387755103), Caption(word='ensures', start=101.01469387755103, end=101.33469387755103), Caption(word='high', start=101.33469387755103, end=101.73469387755102), Caption(word='-quality', start=101.73469387755102, end=102.11469387755103), Caption(word='3D', start=102.11469387755103, end=102.83469387755103), Caption(word='reconstruction.', start=102.83469387755103, end=103.25469387755103)], start=82.49469387755103, end=104.17632653061226),\n",
       " Equation(content='e_s = \\\\mathcal{F}_s(\\\\mathbf{E}_s), e_i = \\\\mathcal{F}_i(\\\\mathbf{E}_i), e_t = \\\\mathcal{F}_t(\\\\mathbf{E}_t)', start=104.17632653061226, end=115.06938775510206),\n",
       " Text(content='This equation shows how the embeddings for shapes, images, and texts are projected into the aligned space. A contrastive loss is applied to ensure these embeddings are well-aligned.', audio=None, audio_path='./audio/text_11.wav', captions=[Caption(word='This', start=104.17632653061226, end=104.35632653061226), Caption(word='equation', start=104.35632653061226, end=104.71632653061226), Caption(word='shows', start=104.71632653061226, end=105.13632653061225), Caption(word='how', start=105.13632653061225, end=105.37632653061226), Caption(word='the', start=105.37632653061226, end=105.49632653061225), Caption(word='embeddings', start=105.49632653061225, end=105.99632653061225), Caption(word='for', start=105.99632653061225, end=106.15632653061226), Caption(word='shapes,', start=106.15632653061226, end=106.51632653061226), Caption(word='images,', start=107.01632653061226, end=107.19632653061225), Caption(word='and', start=107.61632653061226, end=107.63632653061225), Caption(word='texts', start=107.63632653061225, end=107.95632653061226), Caption(word='are', start=107.95632653061226, end=108.41632653061225), Caption(word='projected', start=108.41632653061225, end=108.75632653061226), Caption(word='into', start=108.75632653061226, end=109.27632653061225), Caption(word='the', start=109.27632653061225, end=109.41632653061225), Caption(word='aligned', start=109.41632653061225, end=109.63632653061225), Caption(word='space.', start=109.63632653061225, end=110.17632653061226), Caption(word='A', start=111.01632653061226, end=111.03632653061226), Caption(word='contrast', start=111.03632653061226, end=111.29632653061226), Caption(word='of', start=111.29632653061226, end=111.65632653061226), Caption(word='loss', start=111.65632653061226, end=111.87632653061226), Caption(word='is', start=111.87632653061226, end=112.13632653061225), Caption(word='applied', start=112.13632653061225, end=112.41632653061225), Caption(word='to', start=112.41632653061225, end=112.65632653061226), Caption(word='ensure', start=112.65632653061226, end=112.85632653061225), Caption(word='these', start=112.85632653061225, end=113.17632653061226), Caption(word='embeddings', start=113.17632653061226, end=113.75632653061226), Caption(word='are', start=113.75632653061226, end=113.85632653061225), Caption(word='well', start=113.85632653061225, end=114.03632653061226), Caption(word='aligned.', start=114.03632653061226, end=114.49632653061227)], start=104.17632653061226, end=115.06938775510206),\n",
       " Headline(content='Aligned Shape Latent Diffusion Model (ASLDM)', start=115.06938775510206, end=132.2318367346939),\n",
       " Text(content='ASLDM builds on the success of Latent Diffusion Models. It maps 2D images and texts to the shape latent embeddings within the aligned space. This model leverages a UNet-like architecture to generate consistent and high-quality 3D shapes.', audio=None, audio_path='./audio/text_13.wav', captions=[Caption(word='ASLDM', start=115.06938775510206, end=115.84938775510206), Caption(word='builds', start=115.84938775510206, end=116.06938775510206), Caption(word='on', start=116.06938775510206, end=116.34938775510206), Caption(word='the', start=116.34938775510206, end=116.42938775510206), Caption(word='success', start=116.42938775510206, end=116.78938775510206), Caption(word='of', start=116.78938775510206, end=117.14938775510205), Caption(word='latent', start=117.14938775510205, end=117.44938775510205), Caption(word='diffusion', start=117.44938775510205, end=117.84938775510206), Caption(word='models.', start=117.84938775510206, end=118.48938775510206), Caption(word='It', start=119.22938775510205, end=119.26938775510206), Caption(word='maps', start=119.26938775510206, end=119.50938775510205), Caption(word='2D', start=119.50938775510205, end=120.20938775510206), Caption(word='images', start=120.20938775510206, end=120.58938775510205), Caption(word='and', start=120.58938775510205, end=120.86938775510205), Caption(word='texts', start=120.86938775510205, end=121.12938775510206), Caption(word='to', start=121.12938775510206, end=121.80938775510205), Caption(word='the', start=121.80938775510205, end=121.92938775510206), Caption(word='shape', start=121.92938775510206, end=122.24938775510205), Caption(word='latent', start=122.24938775510205, end=122.54938775510206), Caption(word='embeddings', start=122.54938775510206, end=123.20938775510206), Caption(word='within', start=123.20938775510206, end=123.46938775510206), Caption(word='the', start=123.46938775510206, end=123.66938775510205), Caption(word='aligned', start=123.66938775510205, end=123.86938775510205), Caption(word='space.', start=123.86938775510205, end=124.32938775510206), Caption(word='This', start=125.18938775510206, end=125.26938775510206), Caption(word='model', start=125.26938775510206, end=125.58938775510205), Caption(word='leverages', start=125.58938775510205, end=126.24938775510205), Caption(word='a', start=126.24938775510205, end=126.54938775510206), Caption(word='unit', start=126.54938775510206, end=126.88938775510206), Caption(word='-like', start=126.88938775510206, end=127.30938775510205), Caption(word='architecture', start=127.30938775510205, end=127.72938775510205), Caption(word='to', start=128.21938775510205, end=128.58938775510205), Caption(word='generate', start=128.58938775510205, end=129.00938775510207), Caption(word='consistent', start=129.00938775510207, end=129.46938775510205), Caption(word='and', start=129.46938775510205, end=129.92938775510206), Caption(word='high', start=129.92938775510206, end=130.18938775510205), Caption(word='-quality', start=130.18938775510205, end=130.56938775510207), Caption(word='3D', start=130.56938775510207, end=131.28938775510204), Caption(word='shapes.', start=131.28938775510204, end=131.70938775510206)], start=115.06938775510206, end=132.2318367346939),\n",
       " Equation(content='\\\\mathcal{L} = \\\\| \\\\mathbf{E}_s^{(0)} - \\\\mathbf{E}_s^{(t)} \\\\|', start=132.2318367346939, end=139.15428571428575),\n",
       " Text(content='This objective function ensures the generative process maintains high fidelity throughout the diffusion steps.', audio=None, audio_path='./audio/text_15.wav', captions=[Caption(word='This', start=132.2318367346939, end=132.4118367346939), Caption(word='objective', start=132.4118367346939, end=132.6318367346939), Caption(word='function', start=132.6318367346939, end=133.2918367346939), Caption(word='ensures', start=133.2918367346939, end=133.6518367346939), Caption(word='the', start=133.6518367346939, end=134.0118367346939), Caption(word='generative', start=134.0118367346939, end=134.4718367346939), Caption(word='process', start=134.4718367346939, end=134.9918367346939), Caption(word='maintains', start=134.9918367346939, end=135.6718367346939), Caption(word='high', start=135.6718367346939, end=136.2918367346939), Caption(word='fidelity', start=136.2918367346939, end=136.8118367346939), Caption(word='throughout', start=136.8118367346939, end=137.3118367346939), Caption(word='the', start=137.3118367346939, end=137.6318367346939), Caption(word='diffusion', start=137.6318367346939, end=137.9918367346939), Caption(word='steps.', start=137.9918367346939, end=138.5518367346939)], start=132.2318367346939, end=139.15428571428575),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x3.png', start=139.15428571428575, end=156.83918367346942),\n",
       " Text(content='This figure illustrates the text-conditioned 3D shape generation capability of Michelangelo. Even abstract or detailed textual descriptions result in smooth, high-fidelity 3D shapes, proving the model’s efficiency in capturing and translating semantic details.', audio=None, audio_path='./audio/text_17.wav', captions=[Caption(word='This', start=139.15428571428575, end=139.35428571428574), Caption(word='figure', start=139.35428571428574, end=139.73428571428576), Caption(word='illustrates', start=139.73428571428576, end=140.17428571428576), Caption(word='the', start=140.17428571428576, end=140.59428571428575), Caption(word='text', start=140.59428571428575, end=140.77428571428575), Caption(word='condition', start=140.77428571428575, end=141.23428571428576), Caption(word='3D', start=141.23428571428576, end=142.03428571428574), Caption(word='shape', start=142.03428571428574, end=142.37428571428575), Caption(word='generation', start=142.37428571428575, end=142.79428571428573), Caption(word='capability', start=142.79428571428573, end=143.59428571428575), Caption(word='of', start=143.59428571428575, end=144.01428571428576), Caption(word='Michelangelo.', start=144.01428571428576, end=144.69428571428574), Caption(word='Even', start=145.49428571428575, end=145.59428571428575), Caption(word='abstract', start=145.59428571428575, end=145.97428571428574), Caption(word='or', start=145.97428571428574, end=146.37428571428575), Caption(word='detailed', start=146.37428571428575, end=146.71428571428575), Caption(word='textual', start=146.71428571428575, end=147.29428571428576), Caption(word='descriptions', start=147.29428571428576, end=147.85428571428574), Caption(word='result', start=147.85428571428574, end=148.59428571428575), Caption(word='in', start=148.59428571428575, end=148.83428571428576), Caption(word='smooth,', start=148.83428571428576, end=149.19428571428574), Caption(word='high', start=149.19428571428574, end=149.63428571428574), Caption(word='-fidelity', start=149.63428571428574, end=150.03428571428574), Caption(word='3D', start=150.03428571428574, end=150.89428571428576), Caption(word='shapes,', start=150.89428571428576, end=151.29428571428576), Caption(word='proving', start=151.77428571428575, end=152.23428571428576), Caption(word='the', start=152.23428571428576, end=152.47428571428574), Caption(word=\"model's\", start=152.47428571428574, end=152.89428571428576), Caption(word='efficiency', start=152.89428571428576, end=153.13428571428574), Caption(word='in', start=153.13428571428574, end=153.75428571428574), Caption(word='capturing', start=153.75428571428574, end=154.21428571428575), Caption(word='and', start=154.21428571428575, end=154.53428571428574), Caption(word='translating', start=154.53428571428574, end=155.09428571428575), Caption(word='semantic', start=155.09428571428575, end=155.45428571428576), Caption(word='details.', start=155.45428571428576, end=156.11428571428576)], start=139.15428571428575, end=156.83918367346942),\n",
       " Headline(content='Experimental Validation', start=156.83918367346942, end=177.34530612244902),\n",
       " Text(content='Extensive experiments were conducted using the ShapeNet dataset and a customized 3D Cartoon Monster dataset. Metrics like Intersection Over Union (IoU) and newly proposed Shape-Image Score (SI-S) and Shape-Text Score (ST-S) were used to validate the model’s performance.', audio=None, audio_path='./audio/text_19.wav', captions=[Caption(word='Extensive', start=156.83918367346942, end=157.27918367346942), Caption(word='experiments', start=157.27918367346942, end=157.83918367346942), Caption(word='were', start=157.83918367346942, end=158.11918367346942), Caption(word='conducted', start=158.11918367346942, end=158.45918367346943), Caption(word='using', start=158.45918367346943, end=158.89918367346942), Caption(word='the', start=158.89918367346942, end=159.07918367346943), Caption(word='ShapeNet', start=159.07918367346943, end=159.57918367346943), Caption(word='dataset', start=159.57918367346943, end=159.95918367346943), Caption(word='and', start=159.95918367346943, end=160.79918367346943), Caption(word='a', start=160.79918367346943, end=160.8791836734694), Caption(word='customized', start=160.8791836734694, end=161.21918367346942), Caption(word='3D', start=161.21918367346942, end=162.27918367346942), Caption(word='cartoon', start=162.27918367346942, end=162.67918367346942), Caption(word='monster', start=162.67918367346942, end=163.23918367346943), Caption(word='dataset.', start=163.23918367346943, end=164.01918367346943), Caption(word='Metrics', start=164.51918367346943, end=164.91918367346943), Caption(word='like', start=164.91918367346943, end=165.21918367346942), Caption(word='Intersection', start=165.21918367346942, end=165.8791836734694), Caption(word='Over', start=165.8791836734694, end=166.17918367346942), Caption(word='Union,', start=166.17918367346942, end=166.57918367346943), Caption(word='IU,', start=167.3791836734694, end=167.5391836734694), Caption(word='and', start=168.2591836734694, end=168.51918367346943), Caption(word='newly', start=168.51918367346943, end=168.73918367346943), Caption(word='proposed', start=168.73918367346943, end=169.2591836734694), Caption(word='Shape', start=169.2591836734694, end=169.71918367346942), Caption(word='Image', start=169.71918367346942, end=170.0391836734694), Caption(word='Score,', start=170.0391836734694, end=170.41918367346943), Caption(word='SIS,', start=171.0391836734694, end=171.43918367346942), Caption(word='and', start=172.23918367346943, end=172.23918367346943), Caption(word='Shape', start=172.23918367346943, end=172.47918367346944), Caption(word='Text', start=172.47918367346944, end=172.73918367346943), Caption(word='Score,', start=172.73918367346943, end=173.1991836734694), Caption(word='STS,', start=173.7591836734694, end=174.29918367346943), Caption(word='were', start=174.8191836734694, end=174.91918367346943), Caption(word='used', start=174.91918367346943, end=175.13918367346943), Caption(word='to', start=175.13918367346943, end=175.3191836734694), Caption(word='validate', start=175.3191836734694, end=175.6591836734694), Caption(word='the', start=175.6591836734694, end=175.99918367346942), Caption(word=\"model's\", start=175.99918367346942, end=176.35918367346943), Caption(word='performance.', start=176.35918367346943, end=176.79918367346943)], start=156.83918367346942, end=177.34530612244902),\n",
       " Figure(content='https://ar5iv.labs.arxiv.org/html/2306.17115/assets/x4.png', start=177.34530612244902, end=188.9436734693878),\n",
       " Text(content='Here’s an ablation study showcasing the importance of training the model in the aligned space. Training in this manner leads to samples that are significantly closer to the given text and image conditions.', audio=None, audio_path='./audio/text_21.wav', captions=[Caption(word=\"Here's\", start=177.34530612244902, end=177.64530612244903), Caption(word='an', start=177.64530612244903, end=177.74530612244902), Caption(word='ablation', start=177.74530612244902, end=178.045306122449), Caption(word='study', start=178.045306122449, end=178.52530612244902), Caption(word='showcasing', start=178.52530612244902, end=179.005306122449), Caption(word='the', start=179.005306122449, end=179.385306122449), Caption(word='importance', start=179.385306122449, end=179.68530612244902), Caption(word='of', start=179.68530612244902, end=179.985306122449), Caption(word='training', start=179.985306122449, end=180.30530612244902), Caption(word='the', start=180.30530612244902, end=180.605306122449), Caption(word='model', start=180.605306122449, end=180.86530612244903), Caption(word='in', start=180.86530612244903, end=181.14530612244903), Caption(word='the', start=181.14530612244903, end=181.24530612244902), Caption(word='aligned', start=181.24530612244902, end=181.42530612244903), Caption(word='space.', start=181.42530612244903, end=182.06530612244902), Caption(word='Training', start=182.70530612244903, end=182.985306122449), Caption(word='in', start=182.985306122449, end=183.265306122449), Caption(word='this', start=183.265306122449, end=183.40530612244902), Caption(word='manner', start=183.40530612244902, end=183.70530612244903), Caption(word='leads', start=183.70530612244903, end=184.045306122449), Caption(word='to', start=184.045306122449, end=184.30530612244902), Caption(word='samples', start=184.30530612244902, end=184.725306122449), Caption(word='that', start=184.725306122449, end=185.02530612244902), Caption(word='are', start=185.02530612244902, end=185.12530612244902), Caption(word='significantly', start=185.12530612244902, end=185.545306122449), Caption(word='closer', start=185.545306122449, end=186.18530612244902), Caption(word='to', start=186.18530612244902, end=186.58530612244903), Caption(word='the', start=186.58530612244903, end=186.68530612244902), Caption(word='given', start=186.68530612244902, end=186.92530612244903), Caption(word='text', start=186.92530612244903, end=187.30530612244902), Caption(word='and', start=187.30530612244902, end=187.545306122449), Caption(word='image', start=187.545306122449, end=187.80530612244902), Caption(word='conditions.', start=187.80530612244902, end=188.265306122449)], start=177.34530612244902, end=188.9436734693878),\n",
       " Headline(content='Conclusion and Future Work', start=188.9436734693878, end=223.8171428571429),\n",
       " Text(content='Michelangelo has proven effective in aligning and generating 3D shapes from 2D images and texts. The results show higher quality and diversity of 3D shape generation compared to other methods. Future work could address the need for ground truth 3D shapes for training, possibly leveraging differentiable rendering from multi-view images.', audio=None, audio_path='./audio/text_23.wav', captions=[Caption(word='Michelangelo', start=188.9436734693878, end=189.5236734693878), Caption(word='has', start=189.5236734693878, end=189.82367346938778), Caption(word='proven', start=189.82367346938778, end=190.0236734693878), Caption(word='effective', start=190.0236734693878, end=190.5036734693878), Caption(word='in', start=190.5036734693878, end=190.8836734693878), Caption(word='aligning', start=190.8836734693878, end=191.42367346938778), Caption(word='and', start=191.42367346938778, end=191.5636734693878), Caption(word='generating', start=191.5636734693878, end=192.04367346938778), Caption(word='3D', start=192.04367346938778, end=192.8436734693878), Caption(word='shapes', start=192.8436734693878, end=193.1836734693878), Caption(word='from', start=193.1836734693878, end=193.58367346938778), Caption(word='2D', start=193.58367346938778, end=194.26367346938778), Caption(word='images', start=194.26367346938778, end=194.6836734693878), Caption(word='and', start=194.6836734693878, end=195.04367346938778), Caption(word='texts.', start=195.04367346938778, end=195.3436734693878), Caption(word='The', start=196.2236734693878, end=196.2436734693878), Caption(word='results', start=196.2436734693878, end=196.54367346938778), Caption(word='show', start=196.54367346938778, end=196.9636734693878), Caption(word='higher', start=196.9636734693878, end=197.5236734693878), Caption(word='quality', start=197.5236734693878, end=198.1036734693878), Caption(word='and', start=198.1036734693878, end=198.80367346938777), Caption(word='diversity', start=198.80367346938777, end=199.30367346938777), Caption(word='of', start=199.30367346938777, end=199.7836734693878), Caption(word='3D', start=199.7836734693878, end=200.42367346938778), Caption(word='shape', start=200.42367346938778, end=200.7436734693878), Caption(word='generation', start=200.7436734693878, end=201.1636734693878), Caption(word='compared', start=201.1636734693878, end=201.7836734693878), Caption(word='to', start=201.7836734693878, end=202.1236734693878), Caption(word='other', start=202.1236734693878, end=202.32367346938778), Caption(word='methods.', start=202.32367346938778, end=202.7436734693878), Caption(word='Future', start=203.6036734693878, end=203.6236734693878), Caption(word='work', start=203.6236734693878, end=204.0836734693878), Caption(word='could', start=204.0836734693878, end=204.32367346938778), Caption(word='address', start=204.32367346938778, end=204.5636734693878), Caption(word='the', start=204.5636734693878, end=204.8436734693878), Caption(word='need', start=204.8436734693878, end=205.04367346938778), Caption(word='for', start=205.04367346938778, end=205.2836734693878), Caption(word='ground', start=205.2836734693878, end=205.6636734693878), Caption(word='truth,', start=205.6636734693878, end=206.0036734693878), Caption(word='3D', start=206.32367346938778, end=206.7236734693878), Caption(word='shapes', start=206.7236734693878, end=207.1036734693878), Caption(word='for', start=207.1036734693878, end=207.4636734693878), Caption(word='training,', start=207.4636734693878, end=207.80367346938777), Caption(word='possibly', start=208.3436734693878, end=208.6836734693878), Caption(word='leveraging', start=208.6836734693878, end=209.2236734693878), Caption(word='differentiable', start=209.2236734693878, end=210.0636734693878), Caption(word='rendering', start=210.0636734693878, end=210.4036734693878), Caption(word='from', start=210.4036734693878, end=210.9036734693878), Caption(word='multi', start=210.9036734693878, end=211.1836734693878), Caption(word='-view', start=211.1836734693878, end=211.6036734693878), Caption(word='images.', start=211.6036734693878, end=212.0836734693878)], start=188.9436734693878, end=212.71510204081636),\n",
       " Text(content=\"Thanks for watching Arxflix! Don't forget to like and subscribe for more deep dives into the latest research papers. Leave a comment below on what paper you would like us to cover next!\", audio=None, audio_path='./audio/text_24.wav', captions=[Caption(word='Thanks', start=212.71510204081636, end=212.89510204081637), Caption(word='for', start=212.89510204081637, end=213.13510204081635), Caption(word='watching', start=213.13510204081635, end=213.57510204081638), Caption(word='ARXFlicks!', start=213.57510204081638, end=214.33510204081637), Caption(word=\"Don't\", start=214.99510204081636, end=215.17510204081637), Caption(word='forget', start=215.17510204081637, end=215.29510204081637), Caption(word='to', start=215.29510204081637, end=215.29510204081637), Caption(word='like', start=215.29510204081637, end=215.81510204081636), Caption(word='and', start=215.81510204081636, end=216.01510204081637), Caption(word='subscribe', start=216.01510204081637, end=216.23510204081637), Caption(word='for', start=216.23510204081637, end=216.69510204081635), Caption(word='more', start=216.69510204081635, end=216.87510204081636), Caption(word='deep', start=216.87510204081636, end=217.15510204081636), Caption(word='dives', start=217.15510204081636, end=217.49510204081636), Caption(word='into', start=217.49510204081636, end=217.79510204081637), Caption(word='the', start=217.79510204081637, end=217.95510204081637), Caption(word='latest', start=217.95510204081637, end=218.25510204081635), Caption(word='research', start=218.25510204081635, end=218.51510204081637), Caption(word='papers.', start=218.51510204081637, end=219.13510204081635), Caption(word='Leave', start=219.83510204081637, end=220.01510204081637), Caption(word='a', start=220.01510204081637, end=220.17510204081637), Caption(word='comment', start=220.17510204081637, end=220.47510204081635), Caption(word='below', start=220.47510204081635, end=220.87510204081636), Caption(word='on', start=220.87510204081636, end=221.23510204081637), Caption(word='what', start=221.23510204081637, end=221.41510204081635), Caption(word='paper', start=221.41510204081635, end=221.75510204081635), Caption(word='you', start=221.75510204081635, end=222.05510204081637), Caption(word='would', start=222.05510204081637, end=222.17510204081637), Caption(word='like', start=222.17510204081637, end=222.33510204081637), Caption(word='us', start=222.33510204081637, end=222.55510204081637), Caption(word='to', start=222.55510204081637, end=222.65510204081636), Caption(word='cover', start=222.65510204081636, end=222.89510204081637), Caption(word='next.', start=222.89510204081637, end=223.27510204081636)], start=212.71510204081636, end=223.8171428571429)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_content = [c for c in script_contents if not isinstance(c, Text)]\n",
    "text_content = [c for c in script_contents if isinstance(c, Text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_mp3(text_content: list[Text], out_path: str) -> None:\n",
    "    # Merge all mp3 and add a 0.5s silence between each\n",
    "    audio_all = []\n",
    "    for i, text in enumerate(text_content):\n",
    "        if not text.audio_path:\n",
    "            continue\n",
    "\n",
    "        path = text.audio_path\n",
    "        audio, sr = torchaudio.load(path)\n",
    "        audio_all.append(audio)\n",
    "    audio_all_torch = torch.cat(audio_all, dim=1)\n",
    "    torchaudio.save(out_path, audio_all_torch, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_mp3(text_content, './audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_srt(full_audio_path: str, out_path: str) -> None:    \n",
    "    model = whisper.load_model('base.en')\n",
    "    option = whisper.DecodingOptions(language='en', fp16=True, without_timestamps=False, task='transcribe')\n",
    "    result = model.transcribe(full_audio_path, word_timestamps=True)\n",
    "    flatten_caption = make_caption(result)\n",
    "    \n",
    "    # flatten_caption = []\n",
    "    # for text in all_text_content:\n",
    "    #     if text.captions:\n",
    "    #         for caption in text.captions:\n",
    "    #             flatten_caption.append(caption)\n",
    "        \n",
    "    # flatten_caption = sorted(flatten_caption, key=lambda x: x.start)\n",
    "\n",
    "    subs = [\n",
    "        srt.Subtitle(index=i, start=timedelta(seconds=t.start), end=timedelta(seconds=t.end), content=t.word)\n",
    "        for i, t in enumerate(flatten_caption)\n",
    "    ]\n",
    "    srt_text = srt.compose(subs)\n",
    "    with open(out_path, 'w') as f:\n",
    "        f.write(srt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienblanchon/Git/arxflix/.venv/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "export_srt('./audio.wav', './output.srt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_rich_content_json(rich_content: list[RichContent], out_path: str) -> None:\n",
    "    rich_content_dict = []\n",
    "    for i, content in enumerate(rich_content):\n",
    "        content_dict = {\n",
    "            'type': content.__class__.__name__.lower(),\n",
    "            'content': content.content,\n",
    "            'start': content.start,\n",
    "            'end': content.end\n",
    "        }\n",
    "        rich_content_dict.append(content_dict)\n",
    "    df = pd.DataFrame(rich_content_dict)\n",
    "    df.to_json(out_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_rich_content_json(rich_content, './output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Literal\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_fps = 30\n",
    "video_height = 1080\n",
    "video_width = 1920\n",
    "\n",
    "@dataclass\n",
    "class CompositionProps:\n",
    "    durationInSeconds: int = 5\n",
    "    audioOffsetInSeconds: int = 0\n",
    "    subtitlesFileName: str = \"public/output.srt\"\n",
    "    audioFileName: str = \"public/audio.wav\"\n",
    "    richContentFileName: str = \"public/output.json\"\n",
    "    waveColor: str = \"#a3a5ae\"\n",
    "    subtitlesLinePerPage: int = 2\n",
    "    subtitlesLineHeight: int = 98\n",
    "    subtitlesZoomMeasurerSize: int = 10\n",
    "    onlyDisplayCurrentSentence: bool = True\n",
    "    mirrorWave: bool = False\n",
    "    waveLinesToDisplay: int = 300\n",
    "    waveFreqRangeStartIndex: int = 5\n",
    "    waveNumberOfSamples: Literal['32', '64', '128', '256', '512'] = '512'\n",
    "    durationInFrames: int = field(init=False)\n",
    "    def __post_init__(self):\n",
    "        self.durationInFrames: int = self.durationInSeconds * video_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = CompositionProps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props.durationInFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning: The root directory of your project is /Users/julienblanchon/Git/new_repo/arxflix, but you are executing this command from /Users/julienblanchon/Git/new_repo/arxflix/api. The recommendation is to execute commands from the root directory.\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundling 6%\n",
      "Bundling 17%\n",
      "Bundling 56%\n",
      "Bundling 63%\n",
      "Bundling 69%\n",
      "Bundling 74%\n",
      "Bundling 80%\n",
      "Bundling 85%\n",
      "Bundling 90%\n",
      "Bundling 95%\n",
      "Bundling 100%\n",
      "Copying public dir 16.9 MB\n",
      "Copying public dir 36.9 MB\n",
      "Getting compositions\n",
      "\u001b[90mComposition        Arxflix\u001b[39m\n",
      "\u001b[90mCodec              h264\u001b[39m\n",
      "\u001b[90mOutput             /Users/julienblanchon/Git/new_repo/arxflix/api/output.mp4\u001b[39m\n",
      "\u001b[90mConcurrency        1x\u001b[39m\n",
      "Rendered 0/150\n",
      "Rendered 1/150, time remaining: 2m 44s\n",
      "Rendered 2/150, time remaining: 1m 55s\n",
      "Rendered 3/150, time remaining: 1m 38s\n",
      "Rendered 4/150, time remaining: 30s\n",
      "Rendered 5/150, time remaining: 25s\n",
      "Rendered 6/150, time remaining: 21s\n",
      "Rendered 7/150, time remaining: 19s\n",
      "Rendered 8/150, time remaining: 17s\n",
      "Rendered 9/150, time remaining: 15s\n",
      "Rendered 10/150, time remaining: 14s\n",
      "Rendered 11/150, time remaining: 13s\n",
      "Rendered 12/150, time remaining: 12s\n",
      "Rendered 13/150, time remaining: 12s\n",
      "Rendered 14/150, time remaining: 11s\n",
      "Rendered 15/150, time remaining: 11s\n",
      "Rendered 16/150, time remaining: 10s\n",
      "Rendered 17/150, time remaining: 10s\n",
      "Rendered 18/150, time remaining: 9s\n",
      "Rendered 19/150, time remaining: 9s\n",
      "Rendered 20/150, time remaining: 9s\n",
      "Rendered 21/150, time remaining: 9s\n",
      "Rendered 22/150, time remaining: 8s\n",
      "Rendered 23/150, time remaining: 8s\n",
      "Rendered 24/150, time remaining: 8s\n",
      "Rendered 25/150, time remaining: 8s\n",
      "Rendered 26/150, time remaining: 8s\n",
      "Rendered 27/150, time remaining: 7s\n",
      "Rendered 28/150, time remaining: 7s\n",
      "Rendered 29/150, time remaining: 7s\n",
      "Rendered 30/150, time remaining: 7s\n",
      "Rendered 31/150, time remaining: 7s\n",
      "Rendered 32/150, time remaining: 7s\n",
      "Rendered 33/150, time remaining: 6s\n",
      "Rendered 34/150, time remaining: 6s\n",
      "Rendered 35/150, time remaining: 6s\n",
      "Rendered 36/150, time remaining: 6s\n",
      "Rendered 37/150, time remaining: 6s\n",
      "Rendered 38/150, time remaining: 6s\n",
      "Rendered 39/150, time remaining: 6s\n",
      "Rendered 40/150, time remaining: 6s\n",
      "Rendered 41/150, time remaining: 6s\n",
      "Rendered 42/150, time remaining: 6s\n",
      "Rendered 43/150, time remaining: 5s\n",
      "Rendered 44/150, time remaining: 5s\n",
      "Rendered 45/150, time remaining: 5s\n",
      "Rendered 46/150, time remaining: 5s\n",
      "Rendered 47/150, time remaining: 5s\n",
      "Rendered 48/150, time remaining: 5s\n",
      "Rendered 49/150, time remaining: 5s\n",
      "Rendered 50/150, time remaining: 5s\n",
      "Rendered 51/150, time remaining: 5s\n",
      "Rendered 52/150, time remaining: 5s\n",
      "Rendered 53/150, time remaining: 5s\n",
      "Rendered 54/150, time remaining: 5s\n",
      "Rendered 55/150, time remaining: 4s\n",
      "Rendered 56/150, time remaining: 4s\n",
      "Rendered 57/150, time remaining: 4s\n",
      "Rendered 58/150, time remaining: 4s\n",
      "Rendered 59/150, time remaining: 4s\n",
      "Rendered 60/150, time remaining: 4s\n",
      "Rendered 61/150, time remaining: 4s\n",
      "Rendered 62/150, time remaining: 4s\n",
      "Rendered 63/150, time remaining: 4s\n",
      "Rendered 64/150, time remaining: 4s\n",
      "Rendered 65/150, time remaining: 4s\n",
      "Rendered 66/150, time remaining: 4s\n",
      "Rendered 67/150, time remaining: 4s\n",
      "Rendered 68/150, time remaining: 4s\n",
      "Rendered 69/150, time remaining: 4s\n",
      "Rendered 70/150, time remaining: 4s\n",
      "Rendered 71/150, time remaining: 3s\n",
      "Rendered 72/150, time remaining: 3s\n",
      "Rendered 73/150, time remaining: 3s\n",
      "Rendered 74/150, time remaining: 3s\n",
      "Rendered 75/150, time remaining: 3s\n",
      "Rendered 76/150, time remaining: 3s\n",
      "Rendered 77/150, time remaining: 3s\n",
      "Rendered 78/150, time remaining: 3s\n",
      "Rendered 79/150, time remaining: 3s\n",
      "Rendered 80/150, time remaining: 3s\n",
      "Rendered 81/150, time remaining: 3s\n",
      "Rendered 82/150, time remaining: 3s\n",
      "Rendered 83/150, time remaining: 3s\n",
      "Rendered 84/150, time remaining: 3s\n",
      "Rendered 85/150, time remaining: 3s\n",
      "Rendered 86/150, time remaining: 3s\n",
      "Rendered 87/150, time remaining: 3s\n",
      "Rendered 88/150, time remaining: 3s\n",
      "Rendered 89/150, time remaining: 3s\n",
      "Rendered 90/150, time remaining: 3s\n",
      "Rendered 91/150, time remaining: 3s\n",
      "Rendered 92/150, time remaining: 2s\n",
      "Rendered 93/150, time remaining: 2s\n",
      "Rendered 94/150, time remaining: 2s\n",
      "Rendered 95/150, time remaining: 2s\n",
      "Rendered 96/150, time remaining: 2s\n",
      "Rendered 97/150, time remaining: 2s\n",
      "Rendered 98/150, time remaining: 2s\n",
      "Rendered 99/150, time remaining: 2s\n",
      "Rendered 100/150, time remaining: 2s\n",
      "Rendered 101/150, time remaining: 2s\n",
      "Rendered 102/150, time remaining: 2s\n",
      "Rendered 103/150, time remaining: 2s\n",
      "Rendered 104/150, time remaining: 2s\n",
      "Rendered 105/150, time remaining: 2s\n",
      "Rendered 106/150, time remaining: 2s\n",
      "Rendered 107/150, time remaining: 2s\n",
      "Rendered 108/150, time remaining: 2s\n",
      "Rendered 109/150, time remaining: 2s\n",
      "Rendered 110/150, time remaining: 2s\n",
      "Rendered 111/150, time remaining: 2s\n",
      "Rendered 112/150, time remaining: 2s\n",
      "Rendered 113/150, time remaining: 2s\n",
      "Rendered 114/150, time remaining: 1s\n",
      "Rendered 115/150, time remaining: 1s\n",
      "Rendered 116/150, time remaining: 1s\n",
      "Rendered 117/150, time remaining: 1s\n",
      "Rendered 118/150, time remaining: 1s\n",
      "Rendered 119/150, time remaining: 1s\n",
      "Rendered 120/150, time remaining: 1s\n",
      "Rendered 121/150, time remaining: 1s\n",
      "Rendered 122/150, time remaining: 1s\n",
      "Rendered 123/150, time remaining: 1s\n",
      "Rendered 124/150, time remaining: 1s\n",
      "Rendered 125/150, time remaining: 1s\n",
      "Rendered 126/150, time remaining: 1s\n",
      "Rendered 127/150, time remaining: 1s\n",
      "Rendered 128/150, time remaining: 1s\n",
      "Rendered 129/150, time remaining: 1s\n",
      "Rendered 130/150, time remaining: 1s\n",
      "Rendered 131/150, time remaining: 1s\n",
      "Rendered 132/150, time remaining: 1s\n",
      "Rendered 133/150, time remaining: 1s\n",
      "Rendered 134/150, time remaining: 1s\n",
      "Rendered 135/150, time remaining: 1s\n",
      "Rendered 136/150, time remaining: 1s\n",
      "Rendered 137/150, time remaining: 1s\n",
      "Rendered 138/150, time remaining: 0s\n",
      "Rendered 139/150, time remaining: 0s\n",
      "Rendered 140/150, time remaining: 0s\n",
      "Rendered 141/150, time remaining: 0s\n",
      "Rendered 142/150, time remaining: 0s\n",
      "Rendered 143/150, time remaining: 0s\n",
      "Rendered 144/150, time remaining: 0s\n",
      "Rendered 145/150, time remaining: 0s\n",
      "Rendered 146/150, time remaining: 0s\n",
      "Rendered 147/150, time remaining: 0s\n",
      "Rendered 148/150, time remaining: 0s\n",
      "Rendered 149/150, time remaining: 0s\n",
      "Rendered 150/150\n",
      "Stitched 18/150\n",
      "Stitched 150/150\n",
      "\u001b[34m○                  /Users/julienblanchon/Git/new_repo/arxflix/api/output.mp4\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['npx', 'remotion', 'render', '/Users/julienblanchon/Git/new_repo/arxflix/api/../remotion/index.ts', '--props', '{\"durationInSeconds\": 5, \"audioOffsetInSeconds\": 0, \"subtitlesFileName\": \"public/output.srt\", \"audioFileName\": \"public/audio.wav\", \"richContentFileName\": \"public/output.json\", \"waveColor\": \"#a3a5ae\", \"subtitlesLinePerPage\": 2, \"subtitlesLineHeight\": 98, \"subtitlesZoomMeasurerSize\": 10, \"onlyDisplayCurrentSentence\": true, \"mirrorWave\": false, \"waveLinesToDisplay\": 300, \"waveFreqRangeStartIndex\": 5, \"waveNumberOfSamples\": \"512\", \"durationInFrames\": 300}', '--compositionId', 'Arflix', '--concurrency', '1', '--output', '/Users/julienblanchon/Git/new_repo/arxflix/api/output.mp4'], returncode=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remotion_root_path = Path(\"../remotion/index.ts\")\n",
    "composition_id = \"Arflix\"\n",
    "props = CompositionProps()\n",
    "# json.dumps(asdict(props))\n",
    "concurrency = 1\n",
    "output = Path(\"./output.mp4\")\n",
    "subprocess.run([\n",
    "  \"npx\", \n",
    "  \"remotion\", \n",
    "  \"render\", \n",
    "  remotion_root_path.absolute().as_posix(),\n",
    "  \"--props\", json.dumps(asdict(props)),\n",
    "  \"--compositionId\", composition_id,\n",
    "  \"--concurrency\", str(concurrency),\n",
    "  \"--output\", output.absolute().as_posix()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'durationInSeconds': 5,\n",
       " 'audioOffsetInSeconds': 0,\n",
       " 'subtitlesFileName': 'output.srt',\n",
       " 'audioFileName': 'audio.wav',\n",
       " 'richContentFileName': 'output.json',\n",
       " 'waveColor': '#a3a5ae',\n",
       " 'subtitlesLinePerPage': 2,\n",
       " 'subtitlesLineHeight': 98,\n",
       " 'subtitlesZoomMeasurerSize': 10,\n",
       " 'onlyDisplayCurrentSentence': True,\n",
       " 'mirrorWave': False,\n",
       " 'waveLinesToDisplay': 300,\n",
       " 'waveFreqRangeStartIndex': 5,\n",
       " 'waveNumberOfSamples': '512'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"durationInSeconds\": 5, \"audioOffsetInSeconds\": 0, \"subtitlesFileName\": \"output.srt\", \"audioFileName\": \"audio.wav\", \"richContentFileName\": \"output.json\", \"waveColor\": \"#a3a5ae\", \"subtitlesLinePerPage\": 2, \"subtitlesLineHeight\": 98, \"subtitlesZoomMeasurerSize\": 10, \"onlyDisplayCurrentSentence\": true, \"mirrorWave\": false, \"waveLinesToDisplay\": 300, \"waveFreqRangeStartIndex\": 5, \"waveNumberOfSamples\": \"512\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "if torch.xpu.is_available():\n",
    "    device = \"xpu\"\n",
    "torch_dtype = torch.float16 if device != \"cpu\" else torch.float32\n",
    "\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\").to(device, dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\")\n",
    "\n",
    "prompt = \"Hey, how are you doing today?\"\n",
    "description = \"A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with clear audio quality. She speaks very fast.\"\n",
    "\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids).to(torch.float32)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
